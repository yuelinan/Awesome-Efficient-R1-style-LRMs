<h1 align="center">
 <em>Don't Overthink It</em>: A Survey of Efficient R1-style 
<br>
  Large Reasoning Models
</h1>

This repository is for our paper:

> **[Efficient Reasoning Models: A Survey](https://arxiv.org/abs/2504.10903)** \
> [Sicheng Feng](https://fscdc.github.io/)<sup>1,2</sup>, [Gongfan Fang](https://fangggf.github.io/)<sup>1</sup>, [Xinyin Ma](https://horseee.github.io/)<sup>1</sup>, [Xinchao Wang](https://sites.google.com/site/sitexinchaowang/)<sup>1,*</sup> \
> <sup>1</sup>National University of Singapore, Singapore \
> <sup>2</sup>Nankai University, Tianjin, China \
> <sup>âˆ—</sup>Corresponding author: xinchao@nus.edu.sg
> 
<p align="center"><img src="./figure/taxonomy.png" width="80%" height="80%"></p>

# ğŸ“‹ Overview 

Recently, Large Reasoning Models (LRMs) have gradually become a research hotspot due to their outstanding performance in handling complex tasks. Among them, DeepSeek R1 has garnered significant attention for its exceptional performance and open-source nature, driving advancements in the research of R1-style LRMs. Unlike traditional Large Language Models (LLMs), these models enhance logical deduction and decision-making capabilities during reasoning by incorporating mechanisms such as long chain-of-thought and self-reflection through reinforcement learning. However, with the widespread application of these models, the problem of "*overthinking*" has gradually emerged.  By  reviewing the current research advancements in the field of efficient reasoning methods systematically, we categorize existing works into two main directions based on the number of models involved in the reasoning process: (1) Efficient Reasoning with Single Model, which focuses on improving the reasoning efficiency of individual models; and (2) Efficient Reasoning with Model Collaboration, which explores optimizing reasoning paths through collaboration among multiple models.

<p align="center"><img src="./figure/taxonomy2.png" width="80%" height="80%"></p>

------

# ğŸ“‘ Table of Contents 

- [Selected Papers](#-Selected-Papers)
  - [Efficient Reasoning with Single Model](#-Efficient-Reasoning-with-Single-Model)
    - [Early Exit](#Early-Exit)
    - [CoT Compression](#CoT-Compression)
    - [Adaptive Reasoning](#Adaptive-Reasoning)
    - [Representation Engineering based Efficient Reasoning](#Representation-Engineering-based-Efficient-Reasoning)
  - [Efficient Reasoning with Model Collaboration](#-Efficient-Reasoning-with-Model-Collaboration)
    - [Longâ€“Short Model Collaboration](#Longâ€“Short-Model-Collaboration)
    - [LLM Routing](#LLM-Routing)
    - [Model Consolidation](#Model-Consolidation)
    - [Speculative Decoding](#Speculative-Decoding)
- [Citation](#-Citation)
- [Acknowledgements](#-Acknowledgements)
------

# ğŸ“š Selected Papers [/All Paper](./paper-list.md)

## ğŸŒŸ Efficient Reasoning with Single Model
Efficient reasoning with single model aims to achieve efficient reasoning by optimizing the reasoning process of a single model. This approach focuses on minimizing computational resources and reasoning time while maintaining reasoning accuracy, ensuring that the model can quickly and accurately generate answers. Specific methods include Early Exit, CoT Compression, Adaptive Reasoning , and Representation Engineering-based Efficient Reasoning.

### Early Exit

<details>
  <summary> To Think or Not to Think: Exploring the Unthinking Vulnerability in Large Reasoning Models <a href="https://arxiv.org/pdf/2502.12202" target="_blank">
    [Paper]
</a></summary>

| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| The authors propose the **Monitoring of Thought (MoT)** framework, which dynamically evaluates the necessity of deep reasoning both before and during the inference process of a large reasoning model. Once it determines that the current input or reasoning is sufficient, MoT immediately terminates the reasoning process, thereby avoiding redundant computation and improving efficiency. | ä½œè€…æå‡ºäº†æ€ç»´ç›‘æ§æ¡†æ¶ï¼ˆMonitoring of Thought, MoTï¼‰ï¼Œè¯¥æ¡†æ¶åœ¨æ¨ç†å¤§æ¨¡å‹æ¥æ”¶è¾“å…¥å‰åŠæ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€è¯„ä¼°æ˜¯å¦éœ€è¦è¿›è¡Œæ·±åº¦æ€è€ƒã€‚ä¸€æ—¦åˆ¤æ–­å½“å‰è¾“å…¥æˆ–æ¨ç†å·²è¶³å¤Ÿï¼ŒMoTå³åˆ»ç»ˆæ­¢æ¨ç†è¿‡ç¨‹ï¼Œé¿å…å†—ä½™è®¡ç®—ï¼Œæé«˜æ•ˆç‡ã€‚ |
</details>                                                                                                             


<details>
<summary> Dynamic Early Exit in Reasoning Models <a href="https://arxiv.org/pdf/2504.15895" target="_blank">
    [Paper]
</a></summary>

| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **DEER**, a **training-free dynamic early-exit reasoning** method. DEER first detects pivotal keywords (e.g., *â€œwaitâ€*) in long chain-of-thought (CoT) sequences and replaces them with guiding tokens such as *â€œfinal answerâ€* to prompt the model to generate a **tentative answer** based on the current reasoning context. The **confidence** of the tentative answer is then evaluated: if sufficiently high, the answer is returned directly; otherwise, the model rolls back to the turning point and continues reasoning. To further improve efficiency, DEER introduces a **branch-parallel acceleration strategy** that executes tentative answer evaluation and continued reasoning in parallel, significantly improving inference speed and resource utilization. | è¯¥è®ºæ–‡æå‡ºä¸€ç§**æ— éœ€è®­ç»ƒçš„åŠ¨æ€æ¨ç†æå‰é€€å‡ºæ–¹æ³• DEER**ã€‚è¯¥æ–¹æ³•é¦–å…ˆåœ¨é•¿ CoT ä¸­æ£€æµ‹å…·æœ‰è½¬æŠ˜æ„ä¹‰çš„å…³é”®è¯ï¼ˆå¦‚ â€œwaitâ€ï¼‰ï¼Œå¹¶å°†å…¶æ›¿æ¢ä¸ºâ€œfinal answerâ€ç­‰å¼•å¯¼è¯ï¼Œæç¤ºæ¨¡å‹åŸºäºå½“å‰æ¨ç†å†…å®¹ç”Ÿæˆ**è¯•æ¢æ€§ç­”æ¡ˆ**ã€‚éšåï¼Œè¯„ä¼°è¯¥ç­”æ¡ˆçš„**ç½®ä¿¡åº¦**ï¼šè‹¥ç½®ä¿¡åº¦è¶³å¤Ÿé«˜åˆ™ç›´æ¥è¾“å‡ºï¼Œåä¹‹åˆ™å›é€€è‡³è½¬æŠ˜ç‚¹ç»§ç»­æ¨ç†ã€‚ä¸ºæå‡æ¨ç†æ•ˆç‡ï¼ŒDEERè¿›ä¸€æ­¥è®¾è®¡äº†**åˆ†æ”¯å¹¶è¡ŒåŠ é€Ÿç­–ç•¥**ï¼Œå°†è¯•æ¢æ€§ç­”æ¡ˆè¯„ä¼°ä¸åç»­æ¨ç†å¹¶è¡Œæ‰§è¡Œï¼Œæœ‰æ•ˆæå‡æ•´ä½“æ¨ç†é€Ÿåº¦ä¸èµ„æºåˆ©ç”¨ç‡ã€‚ |
 </details>                                                  




<details>
<summary>ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning <a href="https://arxiv.org/pdf/2505.04881" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes the **CONCISE** framework. The authors identify two typical redundancy patterns in reasoning: (1) **Confidence Deficit**, where the model underestimates its correct intermediate reasoning steps, triggering unnecessary self-reflection; and (2) **Termination Delay**, where the model continues reflecting even after generating the correct answer. To address these issues, CONCISE introduces two techniques: **Confidence Injection**, which inserts high-confidence phrases to boost the model's trust in its intermediate reasoning; and **Early Stopping**, which employs a confidence detector to monitor model certainty and halts generation once a predefined threshold is exceeded. | è¿™ç¯‡è®ºæ–‡æå‡º CONCISE æ¡†æ¶ã€‚ä½œè€…é¦–å…ˆå½’çº³äº†æ¨ç†ä¸­å­˜åœ¨ä¸¤ç§å…¸å‹å†—ä½™æ¨¡å¼ï¼šä¸€æ˜¯ç½®ä¿¡åº¦ä¸è¶³ï¼ˆConfidence Deficitï¼‰ï¼Œå³æ¨¡å‹ä½ä¼°è‡ªèº«æ­£ç¡®æ¨ç†æ­¥éª¤ï¼Œä»è€Œè§¦å‘ä¸å¿…è¦çš„åæ€ï¼›äºŒæ˜¯ç»ˆæ­¢å»¶è¿Ÿï¼ˆTermination Delayï¼‰ï¼Œå³ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆåä»æŒç»­åæ€ï¼Œé€ æˆå†—ä½™ã€‚ä¸ºæ­¤ï¼ŒCONCISEæ¡†æ¶**ç½®ä¿¡åº¦æ³¨å…¥ï¼ˆConfidence Injectionï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ’å…¥é«˜ç½®ä¿¡åº¦çŸ­è¯­æå‡æ¨¡å‹å¯¹ä¸­é—´æ¨ç†çš„ä¿¡ä»»åº¦ã€‚æå‰ç»ˆæ­¢ï¼ˆEarly Stoppingï¼‰æ–¹æ³•é€šè¿‡ç½®ä¿¡åº¦æ¢æµ‹å™¨ç›‘æ§æ¨¡å‹ç½®ä¿¡åº¦ï¼Œåœ¨è¶…è¿‡è®¾å®šé˜ˆå€¼æ—¶ç»ˆæ­¢ç”Ÿæˆã€‚ |
</details>                                                   



<details>
<summary> Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens<a href="https://arxiv.org/pdf/2505.18237" target="_blank">
    [Paper]
</a></summary>


| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper revisits the efficiency of reasoning paths in large language models from an information-theoretic perspective, revealing a fundamental trade-off between *reasoning length* and *semantic efficiency*. The authors introduce two novel metrics: **InfoBias**, which measures the deviation of a modelâ€™s reasoning path from the ideal path, and **InfoGain**, which quantifies the information increment brought by each reasoning step. Empirical studies show that longer reasoning paths exhibit higher InfoBias and diminishing InfoGain, especially when generating incorrect answers. To address this, the paper proposes an **information entropyâ€“based adaptive reasoning mechanism**, which includes a dynamic stopping strategy (terminating reasoning early when InfoGain remains below a threshold for *k* consecutive steps) and entropy-regularized training (introducing an entropy loss during fine-tuning to encourage the model to terminate early under low-entropy conditions). | æœ¬æ–‡ä»ä¿¡æ¯è®ºè§†è§’é‡æ–°å®¡è§†äº†å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†è·¯å¾„çš„æ•ˆç‡é—®é¢˜ï¼Œæ­ç¤ºäº†â€œæ¨ç†é•¿åº¦â€ä¸â€œè¯­ä¹‰æ•ˆç‡â€ä¹‹é—´å­˜åœ¨åŸºæœ¬æƒè¡¡å…³ç³»ã€‚ä½œè€…å¼•å…¥äº†ä¸¤ä¸ªæ–°æŒ‡æ ‡ï¼š**InfoBias**ï¼ˆè¡¡é‡æ¨¡å‹æ¨ç†è·¯å¾„åç¦»ç†æƒ³è·¯å¾„çš„ç¨‹åº¦ï¼‰å’Œ **InfoGain**ï¼ˆè¡¡é‡æ¯æ­¥æ¨ç†æ‰€å¸¦æ¥çš„ä¿¡æ¯å¢é‡ï¼‰ï¼Œå¹¶é€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼šæ¨ç†è·¯å¾„è¶Šé•¿ï¼Œä¿¡æ¯åå·®è¶Šé«˜ã€ä¿¡æ¯å¢ç›Šè¶Šé€’å‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆé”™è¯¯ç­”æ¡ˆæ—¶æ›´ä¸ºæ˜¾è‘—ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§**åŸºäºä¿¡æ¯ç†µçš„è‡ªé€‚åº”æ¨ç†æœºåˆ¶**ï¼ŒåŒ…æ‹¬åŠ¨æ€ç»ˆæ­¢æœºåˆ¶ï¼ˆå½“è¿ç»­kæ­¥çš„InfoGainä½äºé˜ˆå€¼æ—¶ï¼Œæå‰ç»ˆæ­¢æ¨ç†ï¼‰å’Œç†µæ­£åˆ™åŒ–è®­ç»ƒï¼ˆåœ¨å¾®è°ƒé˜¶æ®µå¼•å…¥ç†µæŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ¨¡å‹åœ¨ä½ç†µçŠ¶æ€ä¸‹æå‰ç»ˆæ­¢ï¼‰ã€‚ |
</details>                                               


<details>
<summary> Scalable Chain of Thoughts via Elastic Reasoning<a href="https://arxiv.org/pdf/2505.05315" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes the **Elastic Reasoning** method, which divides the token budget of the reasoning process into two parts: the thinking phase and the answering phase, enabling controllable and adaptive management of reasoning length. The method enforces early termination of the reasoning once the token budget for the thinking phase is reached, ensuring the completeness of the answering phase. | è¿™ç¯‡è®ºæ–‡æå‡ºäº†å¼¹æ€§æ¨ç†ï¼ˆElastic Reasoningï¼‰æ–¹æ³•ï¼Œé€šè¿‡å°†æ¨ç†è¿‡ç¨‹çš„tokené¢„ç®—åˆ†ä¸ºæ€è€ƒé˜¶æ®µå’Œè§£ç­”é˜¶æ®µä¸¤éƒ¨åˆ†ï¼Œå®ç°å¯¹æ¨ç†é•¿åº¦çš„å¯æ§å’Œè‡ªé€‚åº”ç®¡ç†ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ€è€ƒé˜¶æ®µè¾¾åˆ°é¢„ç®—ä¸Šé™æ—¶å¼ºåˆ¶ç»“æŸæ¨ç†ï¼Œä¿è¯äº†è§£ç­”éƒ¨åˆ†çš„å®Œæ•´æ€§ã€‚ |
 </details>                                                


<details>
<summary> Answer Convergence as a Signal for Early Stopping in Reasoning<a href="https://arxiv.org/pdf/2506.02536" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper addresses the issues of verbosity and redundancy in chain-of-thought reasoning of large language models (LLMs) and proposes three early stopping strategies to improve inference efficiency: (1) an unsupervised stopping mechanism based on answer consistency, which detects convergence by monitoring the consistency of consecutive output answers; (2) a decoding strategy that adjusts the probability of generating an â€œend-of-reasoningâ€ token to encourage early termination; and (3) a supervised learning method leveraging internal model activation sequences, where an LSTM models reasoning progress to dynamically predict the optimal stopping point. | è¿™ç¯‡è®ºæ–‡é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é“¾å¼æ€ç»´æ¨ç†ä¸­å­˜åœ¨çš„å†—é•¿å’Œå†—ä½™é—®é¢˜ï¼Œæå‡ºäº†ä¸‰ç§æ¨ç†è¿‡ç¨‹ä¸­çš„æå‰ç»ˆæ­¢ç­–ç•¥ä»¥æå‡æ¨ç†æ•ˆç‡ï¼šåŸºäºç­”æ¡ˆä¸€è‡´æ€§çš„æ— ç›‘ç£åœæ­¢æœºåˆ¶ï¼Œé€šè¿‡ç›‘æµ‹è¿ç»­è¾“å‡ºç­”æ¡ˆçš„ä¸€è‡´æ€§åˆ¤æ–­æ¨ç†æ˜¯å¦æ”¶æ•›ï¼›åŸºäºç»“æŸæ ‡è®°æ¦‚ç‡è°ƒæ•´çš„è§£ç ç­–ç•¥ï¼Œæå‡æ¨¡å‹ç”Ÿæˆâ€œæ¨ç†ç»“æŸâ€æ ‡è®°çš„æ¦‚ç‡ä»¥ä¿ƒä½¿æå‰åœæ­¢ï¼›ä»¥åŠåŸºäºæ¨¡å‹å†…éƒ¨æ¿€æ´»åºåˆ—çš„ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨LSTMå¯¹æ¨ç†è¿›åº¦è¿›è¡Œå»ºæ¨¡ï¼ŒåŠ¨æ€é¢„æµ‹æœ€ä½³åœæ­¢ç‚¹ã€‚ |
</details>                                                



<details>
<summary> Reasoning Models Know When Theyâ€™re Right: Probing Hidden States for Self-Verification<a href="https://arxiv.org/pdf/2504.05419" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper segments the full reasoning process into several chunks, with the model generating an intermediate answer at the end of each chunk. Each intermediate answer is labeled with a binary supervision signal (*y* âˆˆ {0, 1}) indicating whether it corresponds to the final correct answer. The final hidden state of each chunk is extracted as the input feature *x*, forming a training dataset of (*x*, *y*) pairs. A multi-layer perceptron (MLP) probe is then trained to predict the probability that a given intermediate answer is correct. During inference, if the predicted probability of an intermediate answer being correct exceeds a certain threshold, reasoning is terminated early and the answer is output, thereby reducing unnecessary computational overhead. | æœ¬æ–‡å°†å®Œæ•´çš„æ¨ç†è¿‡ç¨‹åˆ’åˆ†ä¸ºè‹¥å¹²ä¸ª chunkï¼Œå¹¶åœ¨æ¯ä¸ª chunk æœ«å°¾ç”±æ¨¡å‹ç”Ÿæˆä¸€ä¸ªä¸­é—´ç­”æ¡ˆï¼Œæ ‡æ³¨å…¶æ˜¯å¦ä¸ºæœ€ç»ˆæ­£ç¡®ç­”æ¡ˆï¼ˆä»¥ 0/1 ä½œä¸ºç›‘ç£ä¿¡å· yï¼‰ã€‚åŒæ—¶ï¼Œæå–æ¯ä¸ª chunk å¯¹åº”çš„æœ€åä¸€ä¸ª hidden state ä½œä¸ºè¾“å…¥ç‰¹å¾ xï¼Œæ„å»ºè®­ç»ƒæ•°æ®é›† (x, y)ã€‚åœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒä¸€ä¸ª MLP æ¢é’ˆï¼Œç”¨äºé¢„æµ‹å½“å‰ç­”æ¡ˆä¸ºæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå½“æ¨¡å‹ç”Ÿæˆçš„æŸä¸€ä¸­é—´ç­”æ¡ˆè¢«åˆ¤å®šä¸ºæ­£ç¡®æ¦‚ç‡è¾ƒé«˜æ—¶ï¼Œå³å¯æå‰ç»ˆæ­¢æ¨ç†å¹¶è¾“å‡ºè¯¥ç­”æ¡ˆï¼Œä»è€Œæœ‰æ•ˆå‡å°‘ä¸å¿…è¦çš„è®¡ç®—å¼€é”€ã€‚ |
 </details>                                                 

#### 

<details>
<summary>FlashThink: An Early Exit Method For Efficient Reasoning <a href="https://arxiv.org/pdf/2505.13949" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| FlashThink designs an early stopping mechanism based on a verification model. This method divides the reasoning content into multiple segments and uses a trained verification model to determine whether the current segment is sufficient to produce the correct answer, thereby deciding whether to terminate reasoning early. | FlashThink è®¾è®¡äº†ä¸€ç§åŸºäºéªŒè¯æ¨¡å‹çš„æ—©åœæœºåˆ¶ã€‚è¯¥æ–¹æ³•å°†æ¨ç†å†…å®¹åˆ’åˆ†ä¸ºå¤šä¸ªç‰‡æ®µï¼Œåˆ©ç”¨è®­ç»ƒå¥½çš„éªŒè¯æ¨¡å‹åˆ¤æ–­å½“å‰æ¨ç†ç‰‡æ®µæ˜¯å¦å·²è¶³å¤Ÿå¾—åˆ°æ­£ç¡®ç­”æ¡ˆï¼Œä»è€Œå†³å®šæ˜¯å¦æå‰ç»ˆæ­¢æ¨ç†ã€‚ |
</details>                                             



<details>
<summary> Wait, We Donâ€™t Need to â€œWaitâ€ ! Removing Thinking Tokens Improves Reasoning Efficiency<a href="https://arxiv.org/pdf/2506.08343" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes the **NOWAIT** method, which employs a logit processor during inference to **effectively prohibit the generation of specific key tokens**. For any designated key tokens, its corresponding logit value is set to a large negative number during generation, making tokens related to self-reflection nearly impossible to be sampled by the model, thereby enabling more efficient reasoning. The suppressed key tokens include:<br/> â€œwait,â€ â€œalternatively,â€ â€œhmm,â€ â€œbut,â€ â€œhowever,â€ â€œalternative,â€ â€œanother,â€ â€œcheck,â€ â€œdouble-check,â€ â€œoh,â€ â€œmaybe,â€ â€œverify,â€ â€œother,â€ â€œagain,â€ â€œnow,â€ â€œah,â€ and â€œany.â€ | æœ¬æ–‡æå‡ºNOWAITæ–¹æ³• ,åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡‡ç”¨ logit å¤„ç†å™¨ï¼Œ**æœ‰æ•ˆç¦æ­¢æ¨¡å‹ç”Ÿæˆç‰¹å®šå…³é”®è¯**ã€‚å¯¹äºä»»æ„æŒ‡å®šå…³é”®è¯ï¼Œç”Ÿæˆæ—¶ç›´æ¥å°†å…¶å¯¹åº”çš„ logit å€¼è®¾ç½®ä¸ºæå¤§è´Ÿæ•°ï¼Œä»è€Œä½¿ä¸åæ€ç›¸å…³çš„ token å‡ ä¹ä¸å¯èƒ½è¢«æ¨¡å‹é‡‡æ ·ï¼Œè¿›è€Œå®ç°é«˜æ•ˆæ¨ç†ã€‚å…³é”®è¯åŒ…æ‹¬ï¼š<br/> â€œwaitâ€, â€œalternativelyâ€, â€œhmmâ€, â€œbutâ€, â€œhoweverâ€, â€œalternativeâ€, â€œanotherâ€, â€œcheckâ€, â€œdouble-checkâ€, â€œohâ€, â€œmaybeâ€, â€œverifyâ€, â€œotherâ€, â€œagainâ€, â€œnowâ€, â€œahâ€, â€œanyâ€ |
 </details>                                             

#### 

<details>
<summary> Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs<a href="https://arxiv.org/pdf/2501.18585" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Wang et al. propose the **Thought Switching Penalty (TIP)**, which adjusts the logits of **tokens associated with thought transitions**. | Wangç­‰äººæå‡ºæ€è·¯åˆ‡æ¢æƒ©ç½šæ–¹æ³•ï¼ˆTIP, Thought Switching Penaltyï¼‰ï¼Œå¯¹**ä¸æ€è·¯åˆ‡æ¢ç›¸å…³çš„ token**çš„ logitsè¿›è¡Œè°ƒæ•´ã€‚ |
 </details>                                               

#### 

<details>
<summary> Efficient Reasoning Through Suppression of Self-Affirmation Reflections in Large Reasoning Models <a href="https://www.arxiv.org/pdf/2506.12353" target="_blank">
    [Paper]
</a></summary>



| English Note | Chinese Note |
| ------------ | ------------ |
|              |              |
</details>  

#### 

<details>
<summary>S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models <a href="https://arxiv.org/pdf/2505.07686" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This method inserts  \textit{*early exit*} instructions at different positions within a single reasoning chain to construct multiple serial reasoning path groups. It then applies a decaying reward strategy based on the exit position: the earlier the model terminates reasoning while still producing a correct answer, the higher the reward it receives. This guides the model to stop reasoning as early as possible without sacrificing accuracy. | S-GRPOï¼ˆSerial-Group Decaying-Reward Policy Optimizationï¼‰æ–¹æ³•é€šè¿‡åœ¨å•æ¡æ€ç»´é“¾ä¸åŒä½ç½®æ’å…¥â€œæå‰é€€å‡ºâ€æŒ‡ä»¤ï¼Œæ„é€ ä¸²è¡Œæ¨ç†è·¯å¾„ç»„ï¼Œå¹¶ç»“åˆåŸºäºé€€å‡ºä½ç½®çš„é€’å‡å¥–åŠ±ç­–ç•¥ï¼Œå¼•å¯¼æ¨¡å‹åœ¨ä¿è¯æ­£ç¡®æ€§çš„å‰æä¸‹å°½æ—©ç»ˆæ­¢æ¨ç†ã€‚ç›¸æ¯”å¹¶è¡Œé‡‡æ ·è·¯å¾„çš„GRPOï¼ŒS-GRPOä»¥æ›´ç²¾ç»†çš„æ–¹å¼å»ºæ¨¡æ¨ç†å……åˆ†æ€§ï¼Œæå‡æ¨ç†æ•ˆç‡ä¸ç­”æ¡ˆå‡†ç¡®æ€§ |
 </details>                         

### CoT Compression

<details>
<summary>Not All Tokens Are What You Need In Thinking <a href="https://arxiv.org/pdf/2505.17827" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes a method called **Conditional Token Selection (CTS)** for compressing chain-of-thought (CoT) reasoning data. CTS trains a reference model to assess the importance of tokens during the reasoning process and dynamically removes less important tokens based on metrics such as perplexity, producing a compressed dataset. The model is then fine-tuned on this compressed dataset to achieve more efficient reasoning capabilities. | æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºConditional Token Selectionï¼ˆCTSï¼‰çš„æ–¹æ³•ï¼Œç”¨äºå‹ç¼©é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ•°æ®ã€‚CTSé€šè¿‡è®­ç»ƒå‚è€ƒæ¨¡å‹è¯„ä¼°æ¨ç†è¿‡ç¨‹ä¸­çš„Tokené‡è¦æ€§ï¼Œå¹¶æ ¹æ®å›°æƒ‘åº¦ç­‰æŒ‡æ ‡åŠ¨æ€å»é™¤ä¸é‡è¦çš„Tokenï¼Œç”Ÿæˆå‹ç¼©åçš„æ•°æ®é›†ã€‚éšåï¼Œæ¨¡å‹åœ¨è¯¥å‹ç¼©æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ã€‚ |
</details> 

#### 

<details>
<summary>TokenSkip: Controllable Chain-of-Thought Compression in LLMs <a href="https://arxiv.org/pdf/2502.12067" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **TokenSkip**, a reasoning compression method that first evaluates the importance of each token during the reasoning process. Based on a preset compression rate, a pruning threshold is determined to retain tokens with importance above the threshold, thereby generating a condensed version of the chain-of-thought (CoT). During training, the authors construct a new training set using the compressed reasoning trajectories for fine-tuning. At inference, TokenSkip enables controllable compression. | è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º **TokenSkip** çš„æ¨ç†å‹ç¼©æ–¹æ³•ï¼Œé¦–å…ˆè¡¡é‡**æ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸ª token çš„é‡è¦æ€§**ï¼Œæ ¹æ®é¢„è®¾çš„**å‹ç¼©ç‡ç¡®å®šè£å‰ªé˜ˆå€¼ï¼Œä¿ç•™é«˜äºé˜ˆå€¼çš„ tokenï¼Œä»è€Œç”Ÿæˆç²¾ç®€ç‰ˆçš„æ¨ç†é“¾ï¼ˆCoTï¼‰ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œä½œè€…åˆ©ç”¨å‹ç¼©åçš„æ¨ç†è½¨è¿¹æ„å»ºæ–°è®­ç»ƒé›†ï¼Œå¹¶è¿›è¡Œå¾®è°ƒã€‚åœ¨æ¨ç†é˜¶æ®µå¯å®ç°å¯æ§å‹ç¼©ã€‚ |
</details> 

#### 

<details>
<summary> Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning<a href="https://arxiv.org/pdf/2505.13866" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Reasoning Path Compression (RPC)** improves inference efficiency by periodically compressing the key-value (KV) cache in large reasoning models. The method leverages attention-based importance scoring to evaluate recently generated tokens and retains only high-impact cache entries, thereby reducing redundant computational overhead. | Reasoning Path Compressionï¼ˆRPCï¼‰é€šè¿‡å‘¨æœŸæ€§å‹ç¼©æ¨ç†å¤§æ¨¡å‹ä¸­çš„é”®å€¼ç¼“å­˜ï¼ˆKV cacheï¼‰æ¥æé«˜æ¨ç†æ•ˆç‡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å¯¹è¿‘æœŸç”Ÿæˆçš„ token è¿›è¡Œé‡è¦æ€§è¯„åˆ†ï¼Œä»…ä¿ç•™é«˜å½±å“åŠ›çš„ç¼“å­˜æ¡ç›®ï¼Œä»è€Œå‡å°‘å†—ä½™è®¡ç®—å¼€é”€ã€‚ |
</details> 

#### 

<details>
<summary>Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping <a href="https://arxiv.org/pdf/2505.08392" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This approach first quantifies the contribution of each token to the final prediction by computing the loss. Then, it introduces a dynamic pruning strategy based on uncertainty: when the modelâ€™s prediction entropy is high, indicating greater task difficulty, pruning is reduced; conversely, when entropy is low, more aggressive pruning is allowed. In addition, an Adaptive N-Constraint mechanism is used to limit the number of consecutively pruned tokens based on the moving average of entropy, preserving the continuity of reasoning. Based on these strategies, a compressed dataset is constructed for retraining the model. | æœ¬æ–‡æå‡ºäº† Adaptive GoGI-Skip æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°ï¼Œé‡åŒ–æ¯ä¸ª token å¯¹æœ€ç»ˆæ¨ç†ç»“æœçš„è´¡çŒ®ï¼Œä»è€Œç²¾å‡†è¯†åˆ«å…³é”®æ¨ç†ä¿¡æ¯ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼•å…¥åŸºäºä¸ç¡®å®šæ€§çš„åŠ¨æ€å‰ªæç­–ç•¥ï¼Œé¦–å…ˆåˆ©ç”¨æ¨¡å‹åœ¨æ¯ä¸€æ­¥çš„é¢„æµ‹ç†µä½œä¸ºä¸ç¡®å®šæ€§æŒ‡æ ‡ï¼šå½“é¢„æµ‹ç†µè¾ƒé«˜ï¼Œè¡¨æ˜æ¨ç†ä»»åŠ¡è¾ƒå¤æ‚ï¼Œç›¸åº”å‡å°‘å‹ç¼©åŠ›åº¦ï¼›è€Œåœ¨é¢„æµ‹ç†µè¾ƒä½æ—¶ï¼Œåˆ™å…è®¸æ›´å¤šçš„å‰ªæã€‚åŒæ—¶ï¼Œæ–¹æ³•é‡‡ç”¨ Adaptive N-Constraintï¼ˆANCï¼‰æœºåˆ¶ï¼Œæ ¹æ®æ»‘åŠ¨çª—å£å†…çš„å¹³å‡é¢„æµ‹ç†µåŠ¨æ€é™åˆ¶è¿ç»­å¯å‰ªæçš„ token æ•°é‡ï¼Œæœ‰æ•ˆä¿éšœæ¨ç†è¿‡ç¨‹çš„è¿è´¯æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºå‹ç¼©æ•°æ®é›†ã€‚ |
</details> 

#### 

<details>
<summary> LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling<a href="https://arxiv.org/pdf/2505.19187" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes the **Perplexity-based Importance Refinement (PIR)** framework, which systematically distinguishes between progressive reasoning steps and functional steps within reasoning chains. Leveraging perplexity-based quantitative metrics, PIR selectively prunes low-importance functional reasoning steps to construct a refined dataset for model fine-tuning, thereby improving reasoning efficiency. | è¯¥è®ºæ–‡æå‡ºäº†PIRï¼ˆPerplexity-based Importance Refinementï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿåˆ’åˆ†æ¨ç†é“¾ä¸­çš„æ¸è¿›å¼æ¨ç†å’ŒåŠŸèƒ½æ€§æ­¥éª¤ï¼Œç»“åˆåŸºäºå›°æƒ‘åº¦çš„é‡åŒ–æŒ‡æ ‡ï¼Œæœ‰é€‰æ‹©åœ°å‰ªé™¤ä½é‡è¦æ€§çš„åŠŸèƒ½æ€§æ¨ç†æ­¥éª¤ï¼Œæ„é€ æ–°çš„æ•°æ®é›†ç”¨äºæ¨¡å‹å¾®è°ƒï¼Œæå‡æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚ |
</details> 

#### 

<details>
<summary> Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models<a href="https://arxiv.org/pdf/2502.13260" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| SPIRIT addresses both few-shot CoT prompting and fine-tuning scenarios. SPIRIT iteratively removes or merges reasoning steps based on perplexity, while designing demonstration refinement or training data optimization strategies to ensure that the resulting reasoning chains remain both concise and semantically coherent. | è¿™ç¯‡è®ºæ–‡æå‡ºäº†SPIRITç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡è¯†åˆ«å’Œå»é™¤æ¨ç†é“¾ä¸­ä¸é‡è¦çš„æ­¥éª¤ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹é“¾å¼æ¨ç†çš„æ•ˆç‡å’Œè¿è´¯æ€§ã€‚é’ˆå¯¹å°‘æ ·æœ¬é“¾å¼æ¨ç†å’Œå¾®è°ƒä¸¤ç§åº”ç”¨åœºæ™¯ï¼ŒSPIRITåˆ†åˆ«è®¾è®¡äº†ç²¾ç‚¼æ¼”ç¤ºç¤ºä¾‹å’Œä¼˜åŒ–è®­ç»ƒæ•°æ®çš„ç­–ç•¥ï¼Œç»“åˆå›°æƒ‘åº¦æŒ‡æ ‡è¿›è¡Œè¿­ä»£å¼æ­¥éª¤ç§»é™¤ä¸åˆå¹¶ï¼Œç¡®ä¿æ¨ç†è¿‡ç¨‹æ—¢ç®€æ´åˆä¿æŒè¯­ä¹‰è¿è´¯ã€‚ |
</details> 

#### 

<details>
<summary> R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search<a href="https://arxiv.org/pdf/2505.16838" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This work proposes a multi-stage reasoning compression method. The method first segments the model-generated solution process into semantically coherent chunks. Then, an auxiliary language model is used to generate multiple simplified candidates for each chunk. A greedy selection strategy is applied across chunks to choose candidates that best balance conciseness and fidelity (measured by low language model loss). The resulting high-quality compressed reasoning dataset is then used to fine-tune the model, enabling more efficient reasoning. | è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§å¤šé˜¶æ®µçš„æ¨ç†å‹ç¼©æ–¹æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆå°†æ¨¡å‹ç”Ÿæˆçš„è§£é¢˜è¿‡ç¨‹åˆ’åˆ†ä¸ºç»“æ„æ¸…æ™°çš„è¯­ä¹‰æ®µï¼ˆchunkï¼‰ï¼Œéšååˆ©ç”¨è¾…åŠ©è¯­è¨€æ¨¡å‹å¯¹æ¯ä¸ªæ®µè½ç”Ÿæˆå¤šä¸ªç®€åŒ–å€™é€‰ç‰ˆæœ¬ã€‚åœ¨æ®µè½ä¹‹é—´é‡‡ç”¨è´ªå¿ƒæœç´¢ç­–ç•¥ï¼Œä¼˜å…ˆé€‰æ‹©åœ¨ç®€æ´æ€§å’Œä¿çœŸæ€§ï¼ˆä½è¯­è¨€æ¨¡å‹æŸå¤±ï¼‰ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡çš„å€™é€‰ï¼Œæœ€ç»ˆæ„å»ºå‡ºå‹ç¼©åçš„é«˜è´¨é‡æ¨ç†æ•°æ®é›†ï¼Œé€šè¿‡å¾®è°ƒæ¨¡å‹å®ç°é«˜æ•ˆæ¨ç†ã€‚ |
</details> 

#### 

<details>
<summary> Donâ€™t Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models<a href="https://arxiv.org/pdf/2505.21765" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes the **Dynamic Thought-mode Optimization (DTO)** framework, which segments the reasoning process into different cognitive mode fragments, evaluates and selectively reinforces positive fragments or prunes negative ones, and constructs preference pairs for preference learning. | è¿™ç¯‡è®ºæ–‡æå‡ºäº†åŠ¨æ€æ€ç»´æ¨¡å¼ä¼˜åŒ–ï¼ˆDTOï¼‰æ¡†æ¶ï¼Œé€šè¿‡å°†æ¨ç†è¿‡ç¨‹åˆ’åˆ†ä¸ºä¸åŒæ€ç»´æ¨¡å¼ç‰‡æ®µï¼Œè¯„ä¼°å¹¶é€‰æ‹©æ€§å¼ºåŒ–æ­£é¢ç‰‡æ®µæˆ–å‰ªæè´Ÿé¢ç‰‡æ®µï¼Œå¹¶æ„å»ºåå¥½æ•°æ®å¯¹ï¼Œè¿›è¡Œåå¥½å­¦ä¹ ã€‚ |
</details> 

#### 

<details>
<summary> Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning<a href="https://arxiv.org/pdf/2505.14582" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes the **Prune-on-Logic** framework, which transforms Chain-of-Thought (CoT) reasoning into a logical graph structure to identify and prune redundant or inefficient logical nodes, enabling structured compression of reasoning paths. Unlike traditional compression methods that rely on token count, Prune-on-Logic focuses on semantic-level effectiveness. A new, compressed dataset is constructed from the pruned paths and used for supervised fine-tuning (SFT), resulting in improved reasoning efficiency. | æœ¬æ–‡æå‡ºäº† Prune-on-Logic æ¡†æ¶ï¼Œé€šè¿‡å°† Chain-of-Thoughtï¼ˆCoTï¼‰è½¬åŒ–ä¸ºé€»è¾‘å›¾ç»“æ„ï¼Œè¯†åˆ«å¹¶å‰ªé™¤å†—ä½™æˆ–ä½æ•ˆçš„é€»è¾‘èŠ‚ç‚¹ï¼Œä»è€Œå®ç°ç»“æ„åŒ–çš„æ€ç»´è·¯å¾„å‹ç¼©ã€‚ä¸ä¼ ç»Ÿçš„åŸºäº token æ•°é‡çš„å‹ç¼©æ–¹æ³•ä¸åŒï¼ŒPrune-on-Logic æ›´å…³æ³¨è¯­ä¹‰å±‚é¢çš„æœ‰æ•ˆæ€§ã€‚åœ¨å‰ªæåæ„å»ºæ–°çš„ç²¾ç®€æ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¯¹æ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œæå‡äº†æ¨ç†æ•ˆç‡ã€‚ |
</details> 

#### 

<details>
<summary> A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings<a href="https://arxiv.org/pdf/2505.24550" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| $A^{*}$-Thought models the reasoning process as a search tree. This method employs bidirectional importance estimation (via bidirectional language modeling) and leverages $A^{*}$ search to optimize reasoning paths, effectively compressing long chains and accelerating LLMs inference. | æœ¬æ–‡æå‡ºåŸºäºA*æœç´¢çš„A*-Thoughtæ¡†æ¶ï¼Œé€šè¿‡å»ºæ¨¡æ¨ç†è¿‡ç¨‹ä¸ºæœç´¢æ ‘å¹¶ç»“åˆåŒå‘é‡è¦æ€§ä¼°è®¡ï¼Œå®ç°å¯¹å¤§å‹æ¨ç†æ¨¡å‹é•¿é“¾æ€ç»´çš„é«˜æ•ˆå‹ç¼©ä¸ä¼˜åŒ–ã€‚ |
</details> 

#### 

<details>
<summary>Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models <a href="https://arxiv.org/pdf/2505.03469" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **LS-Mixture SFT**, a method that rewrites long CoT into concise versions while preserving their underlying reasoning structure. By mixing these concise chains with the original long CoT during supervised fine-tuning, the method effectively reduces redundant reasoning behaviors in the model. | æœ¬æ–‡æå‡ºLS-Mixture SFTæ–¹æ³•ï¼Œé€šè¿‡åœ¨ä¿ç•™æ¨ç†ç»“æ„çš„åŸºç¡€ä¸Šå°†é•¿æ¨ç†é“¾æ”¹å†™ä¸ºç®€æ´ç‰ˆæœ¬ï¼Œå¹¶ä¸åŸå§‹é•¿é“¾æ•°æ®æ··åˆè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä»è€Œæœ‰æ•ˆå‡å°‘æ¨¡å‹çš„å†—ä½™æ¨ç†è¡Œä¸ºã€‚ |
</details> 

#### 

<details>
<summary> AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models<a href="https://arxiv.org/pdf/2505.22662" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes Auto Long-Short Reasoning (AutoL2S), which first constructs training data containing both long-chain and short-chain Chain-of-Thought (CoT) paths, where a special token `<EASY>` is introduced at the beginning of the short-chain CoT to indicate that the question is simple. The large language model is then fine-tuned on this mixed dataset. After fine-tuning, if the model generates the `<EASY>` token during inference, it adopts a simplified reasoning path, thereby enabling dynamic compression of the reasoning process. | æœ¬æ–‡æå‡ºäº†Auto Long-Short Reasoningï¼ˆAutoL2Sï¼‰æ–¹æ³•ï¼Œé¦–å…ˆæ„é€ åŒæ—¶åŒ…å«é•¿é“¾ä¸çŸ­é“¾é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è·¯å¾„çš„è®­ç»ƒæ•°æ®ï¼Œå…¶ä¸­åœ¨çŸ­é“¾CoTçš„æ¨ç†èµ·å§‹å¤„å¼•å…¥ç‰¹æ®Šæ ‡è®°<EASY>ï¼Œç”¨äºæŒ‡ç¤ºè¯¥é—®é¢˜ä¸ºç®€å•é—®é¢˜ã€‚éšåä½¿ç”¨è¯¥æ··åˆæ•°æ®å¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç»è¿‡å¾®è°ƒåï¼Œæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­è‹¥ç”Ÿæˆ<EASY>æ ‡è®°ï¼Œåˆ™é‡‡ç”¨ç®€åŒ–çš„æ¨ç†è·¯å¾„ï¼Œä»è€Œå®ç°å¯¹æ¨ç†è¿‡ç¨‹çš„åŠ¨æ€å‹ç¼©ã€‚ |
</details> 

#### 

<details>
<summary> Self-Training Elicits Concise Reasoning in Large Language Models<a href="https://arxiv.org/pdf/2502.20122" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes a self-training method to refine the reasoning process of large language models. The approach leverages the modelâ€™s own generated reasoning paths and applies strategies such as naive Best-of-N (BoN) sampling, Few-Shot Prompting (FS), and FS-guided BoN sampling (FS-BoN) to select the shortest correct reasoning trajectories. These distilled samples are then used to retrain the original model. Through this self-supervised mechanism, the model internalizes a concise reasoning style, enabling efficient and accurate inference without relying on external prompts or repeated sampling at test time. | æœ¬æ–‡æå‡ºä¸€ç§ç”¨äºç²¾ç‚¼å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„è‡ªè®­ç»ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºæ¨¡å‹è‡ªèº«ç”Ÿæˆçš„æ¨ç†è·¯å¾„ï¼Œç»“åˆæœ´ç´  Best-of-N é‡‡æ ·ã€å°æ ·æœ¬æç¤ºï¼ˆFew-Shot Prompting, FSï¼‰ä»¥åŠå°æ ·æœ¬å¼•å¯¼ä¸‹çš„ BoN é‡‡æ ·ï¼ˆFS-BoNï¼‰ç­‰ç­–ç•¥ï¼Œä»ä¸­é€‰å–æœ€çŸ­ä¸”æ­£ç¡®çš„æ¨ç†è·¯å¾„ï¼Œæ„å»ºç²¾ç®€æ¨ç†æ ·æœ¬é›†ï¼Œå¹¶ç”¨äºå¯¹åŸæ¨¡å‹è¿›è¡Œå†è®­ç»ƒã€‚é€šè¿‡è¿™ä¸€è‡ªæˆ‘ç›‘ç£æœºåˆ¶ï¼Œæ¨¡å‹èƒ½å¤Ÿå†…åŒ–ç®€æ´æ¨ç†æ¨¡å¼ï¼Œä»è€Œåœ¨ä¸ä¾èµ–æµ‹è¯•æ—¶æç¤ºæˆ–é‡å¤é‡‡æ ·çš„å‰æä¸‹ï¼Œå®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„æ¨ç†ã€‚ |
</details> 

#### 

<details>
<summary> Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models<a href="https://arxiv.org/pdf/2506.04210" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Suvra et al. propose a Best-of-N (BoN) sampling strategy to achieve efficient reasoning. The difference lies in that they do not reduce the total token budget for reasoning but instead allocate this budget across N sampled paths, generating multiple reasoning trajectories in parallel and selecting the best-performing path as the final output. | Suvra ç­‰äººæå‡ºäº† BoN é‡‡æ ·ç­–ç•¥ä»¥å®ç°é«˜æ•ˆæ¨ç†ã€‚ä¸åŒä¹‹å¤„åœ¨äºï¼Œä»–ä»¬å¹¶æœªç¼©å‡æ€ç»´ token çš„æ€»é¢„ç®—ï¼Œè€Œæ˜¯å°†è¯¥é¢„ç®—åˆ†é…ç»™ N æ¡é‡‡æ ·è·¯å¾„ï¼Œåˆ©ç”¨å¹¶è¡Œè®¡ç®—ç”Ÿæˆå¤šæ¡æ€è€ƒè·¯å¾„ï¼Œæœ€ç»ˆé€‰æ‹©è¡¨ç°æœ€ä¼˜çš„è·¯å¾„ä½œä¸ºè¾“å‡ºã€‚ |
</details> 

#### 

<details>
<summary> SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models<a href="https://arxiv.org/pdf/2506.05745" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Unlike traditional large reasoning models that perform sequential reasoning, **SPRINT** proposes a parallel reasoning approach to reduce inference time. The method consists of a planner and multiple executors. During inference, the planner generates several plans based on the reasoning context, and the executors execute these plans in parallel. By concurrently handling multiple sub-reasoning tasks, SPRINT accelerates the overall inference process. | ä¸åŒäºä¼ ç»Ÿæ¨ç†å¤§æ¨¡å‹çš„é¡ºåºæ¨ç†ï¼ŒSPRINTæå‡ºäº†å¹¶è¡Œæ¨ç†ä»¥å‡å°‘æ¨ç†æ—¶é—´çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç”±è§„åˆ’å™¨å’Œæ‰§è¡Œå™¨ç»„æˆã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè§„åˆ’å™¨åˆ©ç”¨æ¨ç†ä¸Šä¸‹æ–‡ç”Ÿæˆå¤šä¸ªè§„åˆ’ï¼Œä¹‹åå¤šä¸ªæ‰§è¡Œå™¨å¹¶è¡Œçš„æ‰§è¡Œè¿™äº›è®¡åˆ’ï¼Œé€šè¿‡åŒæ—¶å¤„ç†å¤šä¸ªå­æ¨ç†ä»»åŠ¡ï¼ŒåŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚ |
</details> 

#### 

<details>
<summary> Optimizing Length Compression in Large Reasoning Models<a href="https://arxiv.org/pdf/2506.14755" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper introduces the concept of *invalid thinking* and proposes the **Valid Thinking Rate (VTR)** as a metric for quantification. Building on this, the authors formulate two guiding principlesâ€”**Brevity** and **Sufficiency**â€”and design a post-training method, **LC-R1**, based on GRPO. By jointly leveraging length and compression rewards, LC-R1 encourages the model to terminate redundant reasoning while maintaining sufficient inference quality. | æœ¬æ–‡æå‡ºâ€œæ— æ•ˆæ€è€ƒï¼ˆinvalid thinkingï¼‰â€æ¦‚å¿µï¼Œå¹¶å¼•å…¥**æœ‰æ•ˆæ€è€ƒç‡ï¼ˆValid Thinking Rateï¼‰**æŒ‡æ ‡ç”¨äºé‡åŒ–ã€‚ä½œè€…è¿›ä¸€æ­¥æå‡ºä¸¤ä¸ªå…³é”®åŸåˆ™ï¼š**ç®€æ´æ€§ï¼ˆBrevityï¼‰\**ä¸\**å……åˆ†æ€§ï¼ˆSufficiencyï¼‰**ï¼Œä»¥æ­¤ä¸ºæŒ‡å¯¼ï¼Œè®¾è®¡äº†åŸºäº GRPO çš„åè®­ç»ƒæ–¹æ³• LC-R1ã€‚è¯¥æ–¹æ³•è®¾è®¡äº†é’ˆå¯¹ `</think>` ä»¤ç‰Œçš„å‹ç¼©å¥–åŠ±ï¼Œå¼•å¯¼æ¨¡å‹åœ¨ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆååŠæ—¶ç»ˆæ­¢æ¨ç†ï¼Œä»è€Œå‹ç¼©å†—ä½™å†…å®¹å¹¶æå‡æ¨ç†æ•ˆç‡ã€‚åŒæ—¶ï¼Œè‹¥æ¨¡å‹åœ¨æœªæ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆå‰æå‰ç»ˆæ­¢ï¼Œåˆ™æ–½åŠ æƒ©ç½šï¼Œä»¥é˜²æ­¢è¿‡åº¦å‹ç¼©å¸¦æ¥æ€§èƒ½ä¸‹é™ï¼Œä»è€Œåœ¨å‡†ç¡®æ€§ä¸å‹ç¼©ç‡ä¹‹é—´å®ç°æœ‰æ•ˆæƒè¡¡ã€‚ |
</details> 

#### 

<details>
<summary> Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition<a href="https://arxiv.org/pdf/2505.19788" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **MinD (Multi-Turn Decomposition)**, a multi-turn decomposition reasoning method that restructures traditional long-chain Chain-of-Thought (CoT) reasoning into a structured multi-turn interaction process. Specifically, a strong model is first used to convert original CoT traces into multi-turn dialogue format for training data construction, followed by initialization via supervised fine-tuning (SFT). Then, reinforcement learning with the GRPO algorithm is applied, where the reward function incorporates the number of interaction turns as an optimization target, encouraging the model to complete accurate reasoning in fewer turns, thereby improving both reasoning efficiency and interactive performance. | æœ¬æ–‡æå‡ºå¤šè½®åˆ†è§£æ¨ç†æ–¹æ³• MinDï¼ˆMulti-Turn Decompositionï¼‰ï¼Œå°†ä¼ ç»Ÿé•¿é“¾å¼é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†é‡æ„ä¸ºç»“æ„åŒ–çš„å¤šè½®äº¤äº’è¿‡ç¨‹ã€‚å…·ä½“åšæ³•æ˜¯ï¼Œé¦–å…ˆåˆ©ç”¨å¼ºæ¨¡å‹å°†åŸå§‹ CoT è½¬åŒ–ä¸ºå¤šè½®å¯¹è¯æ ¼å¼ä»¥æ„å»ºè®­ç»ƒæ•°æ®ï¼Œå¹¶é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œåˆå§‹åŒ–ã€‚éšåï¼Œé‡‡ç”¨GRPOç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå¹¶åœ¨å¥–åŠ±è®¾è®¡ä¸­å¼•å…¥äº¤äº’è½®æ•°ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œé¼“åŠ±æ¨¡å‹åœ¨æ›´å°‘çš„è½®æ•°å†…å®Œæˆå‡†ç¡®æ¨ç†ï¼Œä»è€Œæå‡æ¨ç†æ•ˆç‡ä¸äº¤äº’è¡¨ç°ã€‚ |
</details> 

#### 

<details>
<summary>Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching <a href="https://arxiv.org/pdf/2503.05179" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes the **Sketch-of-Thought (SoT)** prompting framework, which guides large language models to generate concise and structured reasoning steps, enhancing reasoning efficiency and semantic accuracy. By incorporating three cognitively inspired reasoning paradigmsâ€”concept chains, chunked symbols, and expert dictionariesâ€”SoT adapts to diverse reasoning tasks. Combined with a lightweight routing model for adaptive paradigm selection, SoT effectively optimizes the modelâ€™s reasoning performance and flexibility. | æœ¬æ–‡æå‡ºäº†Sketch-of-Thoughtï¼ˆSoTï¼‰æç¤ºæ¡†æ¶ï¼Œå¼•å¯¼å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç®€æ´ä¸”ç»“æ„åŒ–çš„æ¨ç†æ­¥éª¤ï¼Œä»¥æé«˜æ¨ç†æ•ˆç‡å’Œè¯­ä¹‰å‡†ç¡®æ€§ã€‚é€šè¿‡ä¸‰ç§è®¤çŸ¥é©±åŠ¨çš„æ¨ç†èŒƒå¼ï¼ˆæ¦‚å¿µé“¾ã€åˆ†å—ç¬¦å·ã€ä¸“å®¶è¯å…¸ï¼‰é€‚é…ä¸åŒæ¨ç†ä»»åŠ¡ï¼Œç»“åˆè½»é‡çº§è·¯ç”±æ¨¡å‹å®ç°è‡ªé€‚åº”èŒƒå¼é€‰æ‹©ï¼ŒSoTæœ‰æ•ˆä¼˜åŒ–äº†æ¨¡å‹çš„æ¨ç†è¡¨ç°å’Œçµæ´»æ€§ã€‚ |
</details> 

#### 

<details>
<summary> Donâ€™t Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning<a href="https://arxiv.org/abs/2505.17813" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Hassid et al.  suggest a strategy where $k$ reasoning paths are generated in parallel, and once the shortest $m$ of them ($k \geq m$) are completed, the generation of the remaining paths is terminated. The answers from the $m$ finished paths are then aggregated via majority voting to select the final reasoning outcome. | ä¸ºæé«˜æ¨ç†æ•ˆç‡ï¼Œè¯¥æ–¹æ³•å¯¹ç»™å®šé—®é¢˜å¹¶è¡Œç”Ÿæˆ $k$ æ¡æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ï¼Œä¸€æ—¦å…¶ä¸­æœ€çŸ­çš„ $m$ æ¡è½¨è¿¹ï¼ˆ$m \leq k$ï¼‰ç”Ÿæˆå®Œæ¯•ï¼Œç«‹å³ç»ˆæ­¢å…¶ä½™ç”Ÿæˆè¿‡ç¨‹ã€‚éšåï¼Œå¯¹è¿™ $m$ æ¡è½¨è¿¹æ‰€å¯¹åº”çš„ç­”æ¡ˆè¿›è¡Œå¤šæ•°æŠ•ç¥¨ï¼Œè‹¥å­˜åœ¨ç¥¨æ•°ç›¸åŒçš„æƒ…å†µï¼Œåˆ™é€‰æ‹©æ€ç»´é“¾é•¿åº¦æœ€çŸ­çš„ç­”æ¡ˆä½œä¸ºæœ€ç»ˆè¾“å‡ºã€‚ç”±äºæ¨ç†è·¯å¾„çš„ç”Ÿæˆæˆæœ¬è¾ƒé«˜ï¼Œè¯¥ç­–ç•¥é€šè¿‡å¹¶è¡Œè§£ç å¹¶åœ¨æ»¡è¶³æœ€å°è½¨è¿¹æ•°åå³åˆ»åœæ­¢ï¼Œæœ‰æ•ˆå‡å°‘äº†è®¡ç®—å¼€é”€ï¼Œå¹¶æ˜¾è‘—ç¼©çŸ­äº†æ¨ç†æ—¶é—´ã€‚ |
</details> 


### Adaptive Reasoning

<details>
<summary>Think Only When You Need with Large Hybrid-Reasoning Models <a href="https://arxiv.org/pdf/2505.14631" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes Large Hybrid Reasoning Models (LHRMs), which are first fine-tuned on a mixture of long- and short-form reasoning data to enable the model to master both reasoning modes. Building on this, the authors introduce **Hybrid Group Policy Optimization**, designed to guide the model in adaptive reasoning. Additionally, a new metric called **Hybrid Accuracy** is proposed to evaluate LHRMsâ€™ ability to select the appropriate reasoning mode effectively. | è¿™ç¯‡è®ºæ–‡æå‡ºäº†å¤§å‹æ··åˆæ¨ç†æ¨¡å‹ï¼ˆLHRMsï¼‰ï¼Œé¦–å…ˆï¼Œé€šè¿‡æ··åˆé•¿çŸ­æ¨ç†ä¸¤ç±»æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä½¿æ¨¡å‹æŒæ¡ä¸¤ç§æ¨ç†æ¨¡å¼çš„èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºæ··åˆç¾¤ç­–ç•¥ä¼˜åŒ–ï¼ˆHybrid Group Policy Optimizationï¼‰ï¼Œæ—¨åœ¨å¼•å¯¼æ¨¡å‹è‡ªé€‚åº”æ¨ç†ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ··åˆå‡†ç¡®ç‡ï¼ˆHybrid Accuracyï¼‰çš„æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°å¤§å‹æ··åˆæ¨ç†æ¨¡å‹ï¼ˆLHRMsï¼‰åœ¨é€‰æ‹©åˆé€‚æ¨ç†æ¨¡å¼ä¸Šçš„èƒ½åŠ›ã€‚ |
</details>

#### 

<details>
<summary> Adaptive Deep Reasoning: Triggering Deep Thinking When Needed<a href="https://arxiv.org/pdf/2505.20101" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes a method that automatically selects between short- and long-chain reasoning paths based on problem complexity. The model is first trained via supervised fine-tuning on a dataset containing both short- and long-chain reasoning samples. Subsequently, reinforcement learning is applied to optimize the model, with a reward mechanism based on group-wise accuracy and a first-token logits loss designed to guide reasoning type selection and generation, respectively. | è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§èƒ½å¤Ÿæ ¹æ®é—®é¢˜å¤æ‚åº¦è‡ªåŠ¨é€‰æ‹©çŸ­é“¾æˆ–é•¿é“¾æ¨ç†è·¯å¾„çš„æ–¹æ³•ï¼Œé¦–å…ˆé€šè¿‡åŒ…å«çŸ­é“¾å’Œé•¿é“¾æ¨ç†æ ·æœ¬çš„ç›‘ç£å¾®è°ƒè®­ç»ƒæ¨¡å‹ã€‚éšåå¼•å…¥å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ï¼Œè®¾è®¡äº†åŸºäºç»„å†…æ­£ç¡®ç‡çš„å¥–åŠ±æœºåˆ¶å’Œé¦– token çš„ logits æŸå¤±ï¼Œç”¨äºåˆ†åˆ«å¼•å¯¼æ¨ç†ç±»å‹çš„é€‰æ‹©ä¸ç”Ÿæˆã€‚ |
</details>

#### 

<details>
<summary>Ada-R1: Hybrid CoT via Bi-Level Adaptive Reasoning Optimization <a href="https://arxiv.org/pdf/2504.21659" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **Ada-R1**, a two-stage adaptive reasoning framework. In the first stage, model merging is used to combine a reasoning model with a standard large language model, enabling the generation of both long and short reasoning paths. In the second stage, a biLevel adaptive reasoning mechanism is introduced: at the **group level**, a preference-guided model selects between long and short reasoning based on the input; at the **instance level**, preference optimization encourages the model to produce more concise reasoning while maintaining accuracy, thereby improving overall inference efficiency. | æœ¬æ–‡æå‡ºAda-R1ï¼Œä¸€ä¸ªä¸¤é˜¶æ®µçš„è‡ªé€‚åº”æ¨ç†æ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡æ¨¡å‹åˆå¹¶ï¼Œç»“åˆé•¿æ¨ç†æ¨¡å‹ä¸å¸¸è§„å¤§æ¨¡å‹ï¼Œä½¿å…¶å…·å¤‡ç”Ÿæˆé•¿çŸ­æ¨ç†è·¯å¾„çš„èƒ½åŠ›ï¼›ç¬¬äºŒé˜¶æ®µå¼•å…¥åŒå±‚è‡ªé€‚åº”æ¨ç†ä¼˜åŒ–æœºåˆ¶ï¼ŒåŒ…æ‹¬ç¾¤ä½“çº§åå¥½å¼•å¯¼æ¨¡å‹ä¾æ®è¾“å…¥é€‰æ‹©é•¿æ¨ç†è¿˜æ˜¯çŸ­æ¨ç†ï¼Œå®ä¾‹çº§åå¥½ä¿ƒä½¿æ¨¡å‹åœ¨ä¿æŒå‡†ç¡®æ€§çš„å‰æä¸‹ç”Ÿæˆæ›´ç®€æ´çš„æ¨ç†ï¼Œä»è€Œæå‡æ¨ç†æ•ˆç‡ã€‚ |
</details>

#### 

<details>
<summary> Dast: Difficulty-adaptive slow-thinking for large reasoning models<a href="https://arxiv.org/pdf/2503.04472" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| The DAST method establishes a mapping between problem difficulty and answer length and introduces the Token Length Budget (TLB) metric. By sampling multiple reasoning paths for the same problem and calculating their TLB values, it constructs preference-based sample pairs. The model is then fine-tuned using the SimPO method to achieve adaptive reasoning capabilities in large models. | DAST æ–¹æ³•é€šè¿‡å»ºç«‹é—®é¢˜éš¾åº¦ä¸å›ç­”é•¿åº¦ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œæå‡º Token Length Budgetï¼ˆTLBï¼‰æŒ‡æ ‡ï¼Œåœ¨å¯¹åŒä¸€é—®é¢˜é‡‡æ ·å¤šæ¡æ¨ç†è·¯å¾„å¹¶è®¡ç®—å…¶ TLB å€¼åï¼Œæ„é€ å‡ºä½“ç°æ¨ç†åå¥½çš„æ ·æœ¬å¯¹ï¼Œå¹¶é‡‡ç”¨ SimPO æ–¹æ³•è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå®ç°å¤§æ¨¡å‹çš„è‡ªé€‚åº”æ¨ç†èƒ½åŠ›ã€‚ |
</details>

#### 

<details>
<summary> Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence<a href="https://arxiv.org/pdf/2505.20325" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Guided by Gut (GG)** leverages intrinsic signals from the language modelâ€™s own generation process (e.g., token-level confidence) to guide reasoning search, eliminating the need for external verifier models. It optimizes confidence estimation via reinforcement learning and employs a self-guided tree search strategy, significantly reducing computational cost while maintaining reasoning quality. | Guided by Gutï¼ˆGGï¼‰åˆ©ç”¨è¯­è¨€æ¨¡å‹è‡ªèº«ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å†…åœ¨ä¿¡å·ï¼ˆå¦‚tokenç½®ä¿¡åº¦ï¼‰å¼•å¯¼æ¨ç†æœç´¢ï¼Œæ— éœ€ä¾èµ–é¢å¤–çš„éªŒè¯æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç½®ä¿¡åº¦ä¼°è®¡ï¼Œå¹¶ç»“åˆè‡ªå¼•å¯¼çš„æ ‘æœç´¢ç­–ç•¥ï¼Œåœ¨ä¿æŒæ¨ç†è´¨é‡çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚ |
</details>

#### 

<details>
<summary> Thinker: Learning to Think Fast and Slow<a href="https://arxiv.org/pdf/2505.21097" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This work proposes a four-step reasoning framework called **Thinker**, which begins with a â€œfast thinkingâ€ phase to quickly generate and verify preliminary answers, thereby saving computational resources. If verification fails, the framework transitions to a â€œslow thinkingâ€ phase for in-depth reasoning to correct erroneous answers. Finally, a â€œsummaryâ€ phase distills the complete reasoning path to reinforce the modelâ€™s intuition and integrative capabilities. | è¯¥å·¥ä½œæå‡ºäº†ä¸€ç§å››æ­¥æ¨ç†æ¡†æ¶â€”â€”Thinkerä»»åŠ¡ï¼Œé€šè¿‡â€œå¿«æ€è€ƒâ€å¿«é€Ÿç”Ÿæˆåˆæ­¥ç­”æ¡ˆå¹¶è¿›è¡ŒéªŒè¯ï¼ŒèŠ‚çœè®¡ç®—èµ„æºï¼›è‹¥éªŒè¯å¤±è´¥ï¼Œåˆ™è¿›å…¥â€œæ…¢æ€è€ƒâ€é˜¶æ®µè¿›è¡Œæ·±å…¥æ¨ç†ä»¥çº æ­£é”™è¯¯ç­”æ¡ˆï¼›æœ€åé€šè¿‡â€œæ€»ç»“â€é˜¶æ®µæç‚¼å®Œæ•´æ¨ç†è·¯å¾„ã€‚åœ¨æ¯ä¸€æ­¥ä¸­éƒ½è®¾è®¡ç›¸åº”çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±æ¥å®ç°è‡ªé€‚åº”æ¨ç†ã€‚ |
</details>

#### 

<details>
<summary> Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning<a href="https://arxiv.org/pdf/2505.16315" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes the **Adaptive Cognition Policy Optimization (ACPO)** framework, which explicitly distinguishes between fast and slow thinking steps by introducing system-aware reasoning tokens such as `<fast_think>` and `<slow_think>`, enabling large-scale reasoning models to dynamically switch between these two cognitive modes. To train the model, the authors first generate multiple candidate answers with diverse length prompts based on a high-quality mathematical reasoning dataset. Then, using fine-grained comparative annotations by GPT-4, key reasoning steps are labeled as slow thinking, while redundant or simplified steps are marked as fast thinking, thereby constructing an explicit reasoning path dataset with alternating fast and slow thinking. Based on this, the model undergoes supervised fine-tuning to establish foundational capabilities, followed by reinforcement learning to optimize the reasoning policy. The reinforcement learning phase incorporates an online token length budget (TLB) reward, which dynamically estimates the reasoning length budget based on sampling success rates, guiding the model to adaptively adjust reasoning length according to task difficulty, thus achieving efficient and accurate cognitive resource allocation. | è¿™ç¯‡è®ºæ–‡æå‡ºäº†Adaptive Cognition Policy Optimizationï¼ˆACPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç³»ç»Ÿæ„ŸçŸ¥æ¨ç†æ ‡è®°ï¼ˆå¦‚<fast_think>å’Œ<slow_think>ï¼‰æ˜¾å¼åŒºåˆ†å¿«æ€è€ƒä¸æ…¢æ€è€ƒæ­¥éª¤ï¼Œå®ç°å¤§è§„æ¨¡æ¨ç†æ¨¡å‹åœ¨ä¸¤ç§æ€ç»´æ¨¡å¼é—´çš„åŠ¨æ€åˆ‡æ¢ã€‚ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œä½œè€…åŸºäºé«˜è´¨é‡æ•°å­¦æ¨ç†æ•°æ®é›†ï¼Œå…ˆä½¿ç”¨å¤šæ ·åŒ–é•¿åº¦æç¤ºç”Ÿæˆå¤šç»„å€™é€‰ç­”æ¡ˆï¼Œå†é€šè¿‡GPT-4ç»†ç²’åº¦å¯¹æ¯”æ ‡æ³¨ï¼Œå°†å…³é”®çš„æ¨ç†æ­¥éª¤æ ‡è®°ä¸ºæ…¢æ€è€ƒï¼Œå†—ä½™æˆ–ç®€ç•¥æ­¥éª¤æ ‡è®°ä¸ºå¿«æ€è€ƒï¼Œä»è€Œæ„å»ºåŒ…å«å¿«æ…¢æ€è€ƒäº¤æ›¿çš„æ˜¾å¼æ¨ç†è·¯å¾„æ•°æ®é›†ã€‚åŸºäºæ­¤ï¼Œå…ˆè¿›è¡Œç›‘ç£å¾®è°ƒå»ºç«‹åŸºç¡€èƒ½åŠ›ï¼Œç»§è€Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨ç†ç­–ç•¥ã€‚å¼ºåŒ–å­¦ä¹ ä¸­è®¾è®¡äº†åœ¨çº¿ä»¤ç‰Œé•¿åº¦é¢„ç®—ï¼ˆTLBï¼‰å¥–åŠ±ï¼Œç»“åˆé‡‡æ ·æˆåŠŸç‡åŠ¨æ€ä¼°è®¡æ¨ç†é•¿åº¦é¢„ç®—ï¼Œå¼•å¯¼æ¨¡å‹æ ¹æ®ä»»åŠ¡éš¾åº¦è‡ªé€‚åº”è°ƒæ•´æ¨ç†é•¿åº¦ï¼Œå®ç°é«˜æ•ˆä¸”ç²¾å‡†çš„è®¤çŸ¥èµ„æºåˆ†é…ã€‚ |
</details>

#### 

<details>
<summary> AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting<a href="https://arxiv.org/pdf/2505.18822" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **AdaCtrl**, a framework that enables large language models to perform adaptive reasoning based on question difficulty. The approach first constructs a training dataset annotated with special tokens such as `[Easy]` and `[Hard]`, and performs cold-start fine-tuning to equip the model with length control capabilities. It then introduces a **difficulty-aware response length reward** and a **difficulty calibration reward**, and applies reinforcement learning to further optimize the modelâ€™s reasoning strategy, achieving efficient and controllable reasoning. | æœ¬æ–‡æå‡º AdaCtrl æ¡†æ¶ï¼Œä½¿å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ ¹æ®é—®é¢˜éš¾åº¦è‡ªé€‚åº”è¿›è¡Œæ¨ç†ã€‚é¦–å…ˆæ„é€ å¸¦æœ‰[Easy]ä¸[Hard]ç­‰ç‰¹æ®Štokençš„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶é€šè¿‡å†·å¯åŠ¨å¾®è°ƒä½¿æ¨¡å‹å…·å¤‡é•¿åº¦æ§åˆ¶èƒ½åŠ›ã€‚éšåå¼•å…¥éš¾åº¦æ„ŸçŸ¥çš„å“åº”é•¿åº¦å¥–åŠ±ä¸éš¾åº¦ä¼°è®¡æ ¡å‡†å¥–åŠ±ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†ç­–ç•¥ï¼Œå®ç°é«˜æ•ˆä¸”å¯æ§çš„æ¨ç†è¿‡ç¨‹ã€‚ |
</details>

#### 

<details>
<summary> Thinkless: LLM Learns When to Think<a href="https://arxiv.org/pdf/2505.13379" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This work proposes **Thinkless**, a reinforcement learning-based hybrid reasoning framework that uses `<think>` and `<short>` tokens generated at the beginning and end of output sequences to control the reasoning mode. The authors design **Decoupled Group Relative Policy Optimization (DeGRPO)**, which jointly rewards both the reasoning mode selection and answer accuracy, encouraging the model to learn adaptive reasoning strategies. | è¿™ç¯‡å·¥ä½œæå‡ºäº†Thinklessï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ··åˆæ¨ç†æ¡†æ¶ï¼Œåˆ©ç”¨è¾“å‡ºåºåˆ—é¦–ä½ç”Ÿæˆçš„<think>å’Œ<short>æ ‡è®°æ¥æ§åˆ¶æ¨ç†æ–¹å¼ã€‚ä½œè€…è®¾è®¡äº†å»è€¦ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆDeGRPOï¼‰ï¼Œé€šè¿‡è”åˆå¯¹æ¨ç†æ¨¡å¼é€‰æ‹©å’Œç­”æ¡ˆæ­£ç¡®ç‡è¿›è¡Œå¥–åŠ±ï¼Œä¿ƒè¿›æ¨¡å‹è‡ªé€‚åº”æ¨ç†ç­–ç•¥çš„å­¦ä¹ ã€‚ |
</details>

#### 

<details>
<summary> Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL<a href="https://arxiv.org/pdf/2505.10832" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **AutoThink**, which first introduces an ellipsis-based prompt to elicit optional reasoning behavior in R1-style models. Building on this, a three-stage reinforcement learning framework is developed: Stage 1 enhances the model's ability to switch between "thinking" and "non-thinking" modes; Stage 2 focuses on improving answer accuracy; and Stage 3 optimizes reasoning efficiency by introducing a **length-aware reward** function. This reward encourages the model to remain concise when correct and to elaborate when incorrect, thereby reducing redundant generation. Overall, AutoThink enables the model to autonomously learn **when** and **how** to reason. | æœ¬æ–‡æå‡ºäº† AutoThinkï¼Œé¦–å…ˆå¼•å…¥çœç•¥å·æç¤ºï¼Œæ¿€å‘ R1-style æ¨¡å‹çš„å¯é€‰æ¨ç†è¡Œä¸ºã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼•å…¥ä¸‰é˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬ï¼šé˜¶æ®µä¸€ï¼šå¼ºåŒ–æ¨¡å‹åœ¨â€œæ€è€ƒâ€ä¸â€œéæ€è€ƒâ€ä¹‹é—´åˆ‡æ¢çš„è¡Œä¸ºæ¨¡å¼ï¼›é˜¶æ®µäºŒï¼šæå‡å‡†ç¡®æ€§ï¼›é˜¶æ®µä¸‰ï¼šä¼˜åŒ–æ¨ç†æ•ˆç‡ï¼Œå¼•å…¥åŸºäºå“åº”é•¿åº¦çš„å¥–åŠ±å‡½æ•°ï¼ˆlength-aware rewardï¼‰ï¼Œé¼“åŠ±æ¨¡å‹åœ¨æ­£ç¡®æ—¶ä¿æŒç®€æ´ã€åœ¨é”™è¯¯æ—¶é€‚å½“å±•å¼€åˆ†æï¼Œè¿›ä¸€æ­¥æ§åˆ¶ç”Ÿæˆå†—ä½™ã€‚ä»è€Œå¼•å¯¼æ¨¡å‹è‡ªä¸»å­¦ä¹ ä½•æ—¶ã€å¦‚ä½•æ¨ç†ã€‚ |
</details>

#### 

<details>
<summary>AdaptThink: Reasoning Models Can Learn When to Think <a href="https://arxiv.org/pdf/2505.13417" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| **AdaptThink** introduces a reinforcement learning-based adaptive reasoning algorithm that guides large language models to dynamically choose between *Thinking* and *NoThinking* modes based on the difficulty of the input question. AdaptThink first encourages the model to prefer the more efficient *NoThinking* mode without sacrificing overall accuracy. In addition, it employs an importance sampling mechanism to balance training samples from both reasoning modes during policy optimization, mitigating the cold-start issue in early training stages. | AdaptThink æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”æ¨ç†ç®—æ³•ï¼Œå¼•å¯¼å¤§æ¨¡å‹æ ¹æ®è¾“å…¥é—®é¢˜çš„éš¾åº¦åŠ¨æ€é€‰æ‹©â€œæ€è€ƒï¼ˆThinkingï¼‰â€æˆ–â€œæ— æ€è€ƒï¼ˆNoThinkingï¼‰â€æ¨¡å¼ã€‚AdaptThinké¦–å…ˆé¼“åŠ±æ¨¡å‹åœ¨ä¸é™ä½æ•´ä½“å‡†ç¡®ç‡çš„å‰æä¸‹é‡‡ç”¨æ›´é«˜æ•ˆçš„â€œæ— æ€è€ƒâ€æ¨ç†ã€‚å…¶æ¬¡é‡‡ç”¨é‡è¦æ€§é‡‡æ ·æœºåˆ¶ï¼Œåœ¨ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ä¸­å¹³è¡¡ä¸¤ç§æ¨ç†æ¨¡å¼çš„æ ·æœ¬ï¼Œç¼“è§£è®­ç»ƒåˆæœŸçš„å†·å¯åŠ¨é—®é¢˜ã€‚ |
</details>

#### 

<details>
<summary> AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning<a href="https://arxiv.org/pdf/2505.11896" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| The authors propose **AdaCoT**, a framework that formulates adaptive reasoning as a multi-objective Pareto optimization problem. By introducing a reward function with penalty coefficients and leveraging the Proximal Policy Optimization (PPO) algorithm, the framework guides the model to dynamically decide whether to trigger Chain-of-Thought (CoT) reasoning based on the complexity of the input. | ä½œè€…æå‡ºäº† AdaCoT æ¡†æ¶ï¼Œå°†è‡ªé€‚åº”æ¨ç†å»ºæ¨¡ä¸ºä¸€ä¸ªå¤šç›®æ ‡çš„å¸•ç´¯æ‰˜ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶é€šè¿‡å¼•å…¥å«æƒ©ç½šç³»æ•°çš„å¥–åŠ±å‡½æ•°ï¼Œç»“åˆ PPO æ–¹æ³•ï¼Œå¼•å¯¼æ¨¡å‹æ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€åˆ¤æ–­æ˜¯å¦è§¦å‘ Chain-of-Thought æ¨ç†ã€‚ |
</details>

#### 

<details>
<summary> Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning<a href="https://arxiv.org/pdf/2505.15154" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes a certainty-based adaptive reasoning framework, **Certainty-based Adaptive Reasoning (CAR)**. The method first fine-tunes the model on a mixed dataset of long and short chain-of-thought (CoT) reasoning to enhance its support for different reasoning modes. During inference, CAR initially generates a concise answer and evaluates the modelâ€™s confidence using perplexity (PPL). By modeling the relationship between PPL and answer correctness with a Gaussian distribution, CAR dynamically determines whether to trigger a more complex, long-form reasoning process, thereby achieving adaptive reasoning. | è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç½®ä¿¡åº¦çš„è‡ªé€‚åº”æ¨ç†æ¡†æ¶â€”â€”Certainty-based Adaptive Reasoningï¼ˆCARï¼‰ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡æ··åˆé•¿çŸ­é“¾å¼æ€ç»´çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒè®­ç»ƒï¼Œæå‡æ¨¡å‹å¯¹ä¸åŒæ¨ç†æ¨¡å¼çš„æ”¯æŒèƒ½åŠ›ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒCARå…ˆç”Ÿæˆç®€çŸ­ç­”æ¡ˆï¼Œå¹¶åˆ©ç”¨å›°æƒ‘åº¦ï¼ˆPPLï¼‰è¯„ä¼°æ¨¡å‹å¯¹ç­”æ¡ˆçš„ç½®ä¿¡åº¦ï¼ŒåŸºäºé«˜æ–¯åˆ†å¸ƒå»ºæ¨¡PPLä¸ç­”æ¡ˆæ­£ç¡®æ€§çš„å…³ç³»ï¼ŒåŠ¨æ€åˆ¤æ–­æ˜¯å¦éœ€è¦å¯åŠ¨å¤æ‚çš„é•¿ç¯‡æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œå®ç°è‡ªé€‚åº”æ¨ç†ã€‚ |
</details>

#### 

<details>
<summary>ARM: Adaptive Reasoning Model <a href="https://arxiv.org/pdf/2505.20258" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes an Adaptive Reasoning Model (ARM) that dynamically selects the optimal reasoning format among four alternatives based on task difficulty, balancing reasoning effectiveness and computational efficiency. The training consists of two stages: supervised fine-tuning to enable the model to master multiple reasoning formats, followed by an improved Group Relative Policy Optimization algorithm (**Ada-GRPO**) to guide efficient reasoning mode selection. This approach addresses the issues of ignoring task difficulty due to uniform reasoning format distribution and efficiency degradation caused by dominance of long-chain reasoning. | æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ¨ç†æ¨¡å‹ï¼ˆARMï¼‰ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éš¾åº¦åŠ¨æ€é€‰æ‹©å››ç§æ¨ç†æ ¼å¼ä¸­çš„æœ€ä¼˜æ–¹æ¡ˆï¼Œå…¼é¡¾æ¨ç†æ•ˆæœå’Œè®¡ç®—æ•ˆç‡ã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒï¼šå…ˆç›‘ç£å¾®è°ƒè®©æ¨¡å‹æŒæ¡å¤šç§æ¨ç†æ ¼å¼ï¼Œå†åˆ©ç”¨æ”¹è¿›çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼ˆAda-GRPOï¼‰å¼•å¯¼æ¨¡å‹é«˜æ•ˆé€‰æ‹©æ¨ç†æ–¹å¼ï¼Œè§£å†³äº†æ¨ç†æ ¼å¼å‡åŒ€åˆ†å¸ƒå¿½è§†ä»»åŠ¡éš¾åº¦åŠé•¿é“¾æ¨ç†ä¸»å¯¼å¯¼è‡´æ•ˆç‡ä¸‹é™çš„é—®é¢˜ã€‚ |
</details>

#### 

<details>
<summary> Interleaved Reasoning for Large Language Models via Reinforcement Learning<a href="https://arxiv.org/abs/2505.19640" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Authors introduce the Interleaved Reasoning framework. Unlike the traditional ``\textit{*think-then-answer*}'' linear paradigm, this method adopts an interleaved generation structure of ``\textit{*thinkingâ€“answeringâ€“thinking*}'', where intermediate informative answers are generated during the reasoning process. These answers serve as both guidance for subsequent steps and as verifiable reward signals, enabling the model to iteratively refine its reasoning and converge toward the correct final answer. | æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒèŒƒå¼ â€”â€” **äº¤é”™å¼æ¨ç†ï¼ˆInterleaved Reasoningï¼‰**ï¼Œå®ƒä½¿å¾—LLMs**èƒ½å¤Ÿåœ¨ä¸ä¾èµ–å¤–éƒ¨å·¥å…·çš„å‰æä¸‹å®ç°â€œæ€è€ƒ-ä½œç­”â€çš„äº¤é”™è¿›è¡Œ**ã€‚äº¤é”™å¼æ¨ç†æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­**ç”Ÿæˆæœ‰ä¿¡æ¯é‡çš„ä¸­é—´ç­”æ¡ˆ**ï¼Œä¸ä»…ä¸ºç”¨æˆ·æä¾›**åŠæ—¶åé¦ˆ**ï¼ˆä»è€Œé™ä½TTFTï¼‰ï¼Œä¹Ÿä¸ºæ¨¡å‹åç»­çš„æ¨ç†æ­¥éª¤æä¾›**å¯éªŒè¯çš„å¥–åŠ±ä¿¡å·**ï¼Œå¼•å¯¼å…¶é€æ­¥è¶‹è¿‘æ­£ç¡®ç­”æ¡ˆã€‚é™¤äº†æ ¼å¼å’Œæ­£ç¡®æ€§å¥–åŠ±ï¼Œè¿˜é¢å¤–è®¡ç®—äº†ä¸­é—´å¥–åŠ±ï¼šç ”ç©¶äº†ä¸‰ç§ä¸»è¦æ–¹å¼ï¼š**å…¨æˆ–æ— ï¼ˆAll-or-Noneï¼‰**ï¼šè¦æ±‚æ‰€æœ‰ä¸­é—´æ­¥éª¤**æŒ‰é¡ºåºå…¨éƒ¨æ­£ç¡®**æ‰èƒ½è·å¾—å¥–åŠ±ï¼› **éƒ¨åˆ†å¾—åˆ†ï¼ˆPartial Creditï¼‰**ï¼šå¯¹äºæ¯ä¸ª**å•ç‹¬æ­£ç¡®çš„ä¸­é—´æ­¥éª¤**ç»™äºˆéƒ¨åˆ†å¥–åŠ±ï¼› **æ—¶é—´æŠ˜æ‰£ï¼ˆTime-Discountedï¼‰**ï¼šå¯¹**è¾ƒæ—©æ­£ç¡®çš„ä¸­é—´æ­¥éª¤ç»™äºˆæ›´é«˜å¥–åŠ±**ï¼Œå¹¶å¯¹**æ‰€æœ‰ä¸­é—´æ­¥éª¤éƒ½æ­£ç¡®**çš„æƒ…å†µç»™äºˆé¢å¤–å¥–åŠ±ã€‚ |
</details>

#### 

<details>
<summary> How Far Are We from Optimal Reasoning Efficiency?<a href="https://arxiv.org/pdf/2506.07104" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes a reasoning efficiency evaluation metric called **Reasoning Efficiency Gap (REG)**, which measures the deviation between accuracy and reasoning length by constructing efficiency frontiers of various large models under different training configurations. Additionally, it introduces a reinforcement learning method named **REO-RL**, which optimizes rewards under a small number of representative token budgets, significantly improving the balance between efficiency and performance in complex reasoning tasks. | æœ¬æ–‡æå‡ºæ¨ç†æ•ˆç‡è¯„ä¼°æŒ‡æ ‡ Reasoning Efficiency Gapï¼ˆREGï¼‰ï¼Œé€šè¿‡æ„å»ºå¤šä¸ªå¤§æ¨¡å‹åœ¨ä¸åŒè®­ç»ƒé…ç½®ä¸‹çš„æ¨ç†æ•ˆç‡å‰æ²¿ï¼Œè¡¡é‡å½“å‰æ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸æ¨ç†é•¿åº¦ä¹‹é—´çš„åå·®ã€‚åŒæ—¶ï¼Œæå‡ºå¼ºåŒ–å­¦ä¹ æ–¹æ³• REO-RLï¼Œé€šè¿‡åœ¨å°‘é‡ä»£è¡¨æ€§ token é¢„ç®—ä¸‹ä¼˜åŒ–å¥–åŠ±ï¼Œæ˜¾è‘—æå‡æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ•ˆç‡ä¸æ€§èƒ½å¹³è¡¡ã€‚ |
</details>

#### 

<details>
<summary>AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control <a href="https://arxiv.org/pdf/2506.20160" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes a lightweight reward mechanism that can be seamlessly integrated into existing reinforcement learning frameworks. By designing a reward function with accuracy-awareness, the method effectively compresses the modelâ€™s reasoning length while preserving answer correctness and semantic quality. | æå‡ºäº†ä¸€ç§è½»é‡çº§å¥–åŠ±æœºåˆ¶ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆè‡³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¾è®¡å…·å¤‡å‡†ç¡®æ€§æ„ŸçŸ¥èƒ½åŠ›çš„å¥–åŠ±å‡½æ•°ï¼Œæœ‰æ•ˆå‹ç¼©æ¨¡å‹çš„æ¨ç†é•¿åº¦ï¼ŒåŒæ—¶ä¿æŒç»“æœçš„å‡†ç¡®æ€§ä¸è¯­ä¹‰è´¨é‡ã€‚ |
</details>

#### 

<details>
<summary> O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning<a href="https://arxiv.org/abs/2501.12570" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| To address the redundancy issue in long-chain reasoning of large models, this paper proposes a length-aligned fine-tuning method, **O1-Pruner**, which introduces a *length alignment reward* to encourage the generation of more concise and efficient reasoning paths without sacrificing accuracy. The method integrates this reward into a reinforcement learning framework and adopts an off-policy training strategy inspired by PPO, effectively reducing inference overhead. | æœ¬æ–‡é’ˆå¯¹é•¿æ€ç»´é“¾å¤§æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„å†—ä½™é—®é¢˜ï¼Œæå‡ºä¸€ç§é•¿åº¦åè°ƒå¾®è°ƒæ–¹æ³•ï¼ˆO1-Prunerï¼‰ï¼Œé€šè¿‡å¼•å…¥é•¿åº¦åè°ƒå¥–åŠ±ï¼Œåœ¨ä¿æŒå‡†ç¡®ç‡çš„å‰æä¸‹é¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´ç®€æ´é«˜æ•ˆçš„æ¨ç†è·¯å¾„ã€‚è¯¥æ–¹æ³•å°†è¯¥å¥–åŠ±èå…¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¹¶é‡‡ç”¨å—PPOå¯å‘çš„ç¦»ç­–ç•¥è®­ç»ƒç­–ç•¥ï¼Œæœ‰æ•ˆå‡å°‘æ¨ç†å¼€é”€ã€‚ |
</details>

#### 

<details>
<summary> Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning<a href="https://arxiv.org/pdf/2504.01296" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Thinkprune proposes a reinforcement learning method based on length pruning. Specifically, a length constraint is incorporated into the reward function, where positive feedback is given only to outputs that remain correct after being pruned to a maximum length. To avoid drastic performance drops, Thinkprune adopts an iterative pruning strategy that gradually shortens the reasoning path, thereby improving reasoning efficiency. | Thinkprune æå‡ºäº†ä¸€ç§åŸºäºé•¿åº¦è£å‰ªçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚å…·ä½“åšæ³•æ˜¯åœ¨å¥–åŠ±å‡½æ•°ä¸­åŠ å…¥é•¿åº¦é™åˆ¶ï¼Œä»…å¯¹æœ€å¤§é•¿åº¦è£å‰ªåä»èƒ½å¾—å‡ºæ­£ç¡®ç­”æ¡ˆçš„è¾“å‡ºç»™äºˆæ­£åé¦ˆã€‚ä¸ºé¿å…æ€§èƒ½éª¤é™ï¼ŒThinkprune é‡‡ç”¨è¿­ä»£å¼é•¿åº¦å‰ªæç­–ç•¥ï¼Œé€æ­¥ç¼©çŸ­æ¨ç†è·¯å¾„ï¼Œä»è€Œæå‡æ¨ç†æ•ˆç‡ã€‚ |
</details>

#### 

<details>
<summary>Learn to Reason Efficiently with Adaptive Length-based Reward Shaping <a href="https://arxiv.org/pdf/2505.15612" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **Length-based step reward shaping method  (LASER)**, a reinforcement learning-based reward shaping method that employs a stepwise reward function based on target length. Additionally, it introduces a dynamic and difficulty-aware reward mechanism that jointly considers reasoning length and problem difficulty, achieving a balance between reasoning efficiency and performance. | æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„é•¿åº¦å¥–åŠ±å¡‘å½¢æ–¹æ³•ï¼ˆLASERï¼‰ï¼Œé€šè¿‡åŸºäºç›®æ ‡é•¿åº¦çš„é˜¶æ¢¯å‡½æ•°è®¾è®¡å¥–åŠ±ï¼ŒåŒæ—¶å¼•å…¥åŠ¨æ€ä¸”éš¾åº¦æ„ŸçŸ¥çš„å¥–åŠ±æœºåˆ¶ï¼Œç»¼åˆè€ƒè™‘æ¨ç†é•¿åº¦ä¸é—®é¢˜éš¾åº¦ï¼Œå®ç°æ¨ç†æ•ˆç‡ä¸æ€§èƒ½çš„å¹³è¡¡ã€‚ |
</details>

#### 

<details>
<summary>ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning <a href="https://arxiv.org/pdf/2504.21370" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper hypothesizes the existence of an implicit optimal reasoning length in reasoning models and approximates it using the Sample Optimal Length (SOL). For each problem, multiple candidate answers are generated through *n* rollouts, and the shortest correct answer length is defined as the SOL. A reward function is designed based on the SOL, and optimization is performed using the GRPO algorithm. | è¿™ç¯‡è®ºæ–‡å‡è®¾æ¨ç†æ¨¡å‹éšå¼åœ°å­˜åœ¨ä¸€ä¸ªæœ€ä¼˜æ¨ç†é•¿åº¦ï¼Œä¸ºæ­¤ä½¿ç”¨æ ·æœ¬æœ€ä¼˜é•¿åº¦ï¼ˆSOLï¼‰è¿›è¡Œè¿‘ä¼¼ã€‚å¯¹äºæ¯ä¸ªé—®é¢˜ï¼Œç”Ÿæˆè‹¥å¹²æ¡å€™é€‰å›ç­”ï¼ˆnæ¬¡é‡‡æ ·ï¼‰ï¼Œå¹¶å°†æœ€çŸ­çš„æ­£ç¡®å›ç­”é•¿åº¦å®šä¹‰ä¸ºå…¶SOLã€‚åŸºäºSOLè®¾è®¡å¥–åŠ±å‡½æ•°ï¼Œå¹¶ä½¿ç”¨GRPOç®—æ³•è¿›è¡Œä¼˜åŒ–ã€‚ |
</details>

#### 

<details>
<summary> Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning<a href="https://arxiv.org/pdf/2506.05256" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| During the reinforcement learning training process, this paper introduces an **Adaptive Length Penalty (ALP)**. ALP conducts multiple rollouts for each input question to estimate its **empirical solve rate**, defined as the proportion of successful answers. Based on this solve rate, ALP adaptively adjusts the length penalty: for questions with a high solve rate (i.e., relatively easy questions), a stronger penalty is applied to suppress unnecessarily lengthy reasoning; whereas for questions with a low solve rate (i.e., more difficult ones), the penalty is relaxed to allow longer reasoning chains, thereby improving accuracy. | åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ª**è‡ªé€‚åº”é•¿åº¦æƒ©ç½šé¡¹**ï¼ˆAdaptive Length Penalty, ALPï¼‰ã€‚ALP é€šè¿‡å¯¹æ¯ä¸ªè¾“å…¥é—®é¢˜è¿›è¡Œå¤šæ¬¡å°è¯•ï¼ˆrolloutsï¼‰ï¼Œè®¡ç®—è¯¥é—®é¢˜çš„**ç»éªŒæ±‚è§£ç‡**ï¼ˆsolve rateï¼‰ï¼Œå³æˆåŠŸå›ç­”çš„æ¯”ä¾‹ã€‚æ ¹æ®è¿™ä¸€æ±‚è§£ç‡ï¼ŒALP è‡ªé€‚åº”åœ°è°ƒæ•´é•¿åº¦æƒ©ç½šï¼šå¯¹äºæ±‚è§£ç‡è¾ƒé«˜çš„é—®é¢˜ï¼ˆå³ç›¸å¯¹ç®€å•çš„é—®é¢˜ï¼‰ï¼Œæ–½åŠ æ›´å¼ºçš„é•¿åº¦æƒ©ç½šï¼Œä»¥æŠ‘åˆ¶å†—é•¿æ¨ç†ï¼›è€Œå¯¹äºæ±‚è§£ç‡è¾ƒä½çš„é—®é¢˜ï¼ˆå³è¾ƒéš¾çš„é—®é¢˜ï¼‰ï¼Œåˆ™å‡å¼±æƒ©ç½šï¼Œå…è®¸æ¨¡å‹ç”Ÿæˆæ›´é•¿çš„æ¨ç†é“¾æ¡ä»¥æé«˜æ­£ç¡®ç‡ã€‚ |
</details>

#### 

<details>
<summary> SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning<a href="https://arxiv.org/pdf/2505.11274" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **SelfBudgeter**, a reasoning method capable of self-regulating and allocating its token budget during inference. SelfBudgeter estimates the minimum number of tokens required to generate a correct answer under unrestricted conditions and strictly adheres to either the self-estimated or user-specified token budget during reasoning. The method involves two training stages: in the cold-start stage, the model is fine-tuned to predict the required token count before producing an answer; then, reinforcement learning with the GRPO algorithm is applied to minimize budget usage while maintaining accuracy, ensuring that the output length aligns with the allocated budget. | è¯¥è®ºæ–‡æå‡ºäº† **SelfBudgeter**ï¼Œä¸€ç§å…·å¤‡**è‡ªæˆ‘æ¨ç†é¢„ç®—åˆ†é…ä¸æ§åˆ¶èƒ½åŠ›**çš„æ¨ç†æ–¹æ³•ã€‚SelfBudgeter èƒ½å¤Ÿåœ¨ä¸è®¾é™åˆ¶æ—¶ä¼°ç®—ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆæ‰€éœ€çš„æœ€å° token æ•°ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸¥æ ¼éµå¾ªè‡ªä¼°æˆ–ç”¨æˆ·æŒ‡å®šçš„ token é¢„ç®—ã€‚è¯¥æ–¹æ³•åŒ…å«ä¸¤ä¸ªè®­ç»ƒé˜¶æ®µï¼šé¦–å…ˆåœ¨å†·å¯åŠ¨é˜¶æ®µå¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶åœ¨è¾“å‡ºè§£ç­”å‰é¢„æµ‹æ‰€éœ€ token æ•°ï¼›éšåé‡‡ç”¨ GRPO å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¼˜åŒ–æ¨¡å‹ï¼Œåœ¨ä¿è¯æ­£ç¡®ç‡çš„å‰æä¸‹å®ç°é¢„ç®—æœ€å°åŒ–ï¼Œå¹¶ç¡®ä¿è¾“å‡ºé•¿åº¦ä¸é¢„ç®—ä¿æŒä¸€è‡´ã€‚ |
</details>

#### 

<details>
<summary> When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning<a href="https://arxiv.org/pdf/2505.15400" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **Adaptive Self-Recovery Reasoning (ASRR)**, a method that combines explicit reasoning suppression with an implicit self-recovery module. For simple inputs of low difficulty, the model is guided by a special prefix to directly output the final answer. For more complex inputs, latent reasoning paths are automatically activated. Additionally, a dynamic conditional penalty mechanism is introduced to gradually reduce redundant reasoning while maintaining accuracy, enabling concise and accurate reasoning outputs. | æœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”è‡ªæ¢å¤æ¨ç†ï¼ˆAdaptive Self-Recovery Reasoningï¼ŒASRRï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡æ˜¾å¼æ¨ç†æŠ‘åˆ¶ä¸éšå¼è‡ªæ¢å¤æ¨¡å—ï¼Œå½“é‡åˆ°éš¾åº¦è¾ƒä½çš„ç®€å•è¾“å…¥ï¼Œåˆ©ç”¨ç‰¹æ®Šå‰ç¼€å¼•å¯¼æ¨¡å‹ç›´æ¥è¾“å‡ºæœ€ç»ˆç­”æ¡ˆã€‚å¯¹äºéš¾åº¦è¾ƒé«˜çš„å¤æ‚è¾“å…¥ï¼Œè‡ªåŠ¨æ¿€æ´»æ½œåœ¨çš„æ¨ç†è·¯å¾„ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªåŠ¨æ€æ¡ä»¶æƒ©ç½šæœºåˆ¶ï¼Œåœ¨ç¡®ä¿å‡†ç¡®æ€§çš„åŸºç¡€ä¸Šï¼Œé€æ­¥å‡å°‘å†—ä½™æ¨ç†ï¼Œå®ç°â€œç®€çŸ­ä¸”å‡†ç¡®â€çš„æ¨ç†è¾“å‡ºã€‚ |
</details>

#### 

<details>
<summary>TL;DR: Too Long, Do Re-weighting for Efficient LLM Reasoning Compression <a href="https://arxiv.org/pdf/2506.02678" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper dynamically adjusts the ratio of short-chain and long-chain reasoning data during model fine-tuning, achieving a significant improvement in reasoning efficiency while maintaining high reasoning accuracy. | æœ¬æ–‡åœ¨æ¨¡å‹å¾®è°ƒçš„è¿‡ç¨‹ä¸­ï¼ŒåŠ¨æ€è°ƒæ•´çŸ­é“¾å’Œé•¿é“¾æ¨ç†æ•°æ®çš„æ¯”ä¾‹ï¼Œå®ç°æ¨¡å‹åœ¨ä¿æŒé«˜æ¨ç†å‡†ç¡®æ€§çš„åŒæ—¶å¤§å¹…æå‡æ¨ç†æ•ˆç‡ã€‚ |
</details>

### Representation Engineering based Efficient Reasoning

<details>
<summary>Steerable Reasoning Calibration of Large Language Models for Free <a href="https://arxiv.org/pdf/2504.07986" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| **SEAL (Steerable rEAsoning caLibration)** is a training-free method for improving reasoning efficiency. It classifies reasoning units in the large language model's reasoning process into execution, reflection, and transition types, and constructs a **reasoning guidance vector** to represent the direction of efficient reasoning. During decoding, SEAL refines the hidden space representations in real time to dynamically suppress redundant reflections and abrupt transitions, while preserving essential execution logic. | SEALï¼ˆSteerable rEAsoning caLibrationï¼‰æ˜¯ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒçš„æ¨ç†æ•ˆç‡æå‡æ–¹æ³•ï¼Œé€šè¿‡å¯¹å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„æ€ç»´å•å…ƒè¿›è¡Œåˆ†ç±»ï¼ˆæ‰§è¡Œã€åæ€ã€è¿‡æ¸¡ï¼‰ï¼Œæ„é€ â€œæ¨ç†å¼•å¯¼å‘é‡â€ä»¥è¡¨ç¤ºé«˜æ•ˆæ¨ç†æ–¹å‘ï¼Œå¹¶åœ¨æ¨ç†è§£ç æ—¶å¯¹éšç©ºé—´è¡¨ç¤ºè¿›è¡Œå¾®è°ƒï¼Œä»è€ŒåŠ¨æ€æŠ‘åˆ¶å†—ä½™çš„åæ€ä¸è·³è·ƒæ€§æ¨ç†ï¼Œä¿ç•™å…³é”®æ‰§è¡Œé€»è¾‘ã€‚ |
</details>                                       

#### 

<details>
<summary> On Reasoning Strength Planning in Large Reasoning Models<a href="https://arxiv.org/pdf/2506.08390" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| We investigate this phenomenon from the perspective of model activations. Our analysis reveals that LRMs pre-plan the *reasoning intensity* in their activations before generating any reasoning content, and this intensity is causally governed by the magnitude of a pre-allocated *directional vector*. Specifically, using linear probes, we show that the number of reasoning tokens can be predicted solely from the activation patterns of the input question, suggesting that the model is capable of estimating the required reasoning effort in advance. Further analysis indicates that LRMs encode this reasoning intensity via a directional vector embedded in the activation space, where the vectorâ€™s magnitude modulates the intensity. Subtracting this vector reduces both the number of reasoning tokens and final performance, while adding it leads to longer reasoning traces and improved accuracy. Moreover, this directional vector consistently predicts the reasoning length and influences the duration of the reasoning phase by modifying the logits of the termination token (</think>). | æœ¬æ–‡ä»æ¨¡å‹æ¿€æ´»çš„è§†è§’å¯¹æ­¤ç°è±¡è¿›è¡Œäº†æ¢è®¨ã€‚æˆ‘ä»¬å‘ç°ï¼ŒLRMsåœ¨ç”Ÿæˆæ¨ç†å†…å®¹ä¹‹å‰ï¼Œå·²åœ¨æ¿€æ´»ä¸­é¢„å…ˆè§„åˆ’äº†æ¨ç†å¼ºåº¦ï¼Œè€Œè¯¥æ¨ç†å¼ºåº¦ç”±ä¸€ä¸ªé¢„å…ˆåˆ†é…çš„æ–¹å‘å‘é‡çš„å¤§å°å› æœæ§åˆ¶ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬åˆ©ç”¨çº¿æ€§æ¢é’ˆè¡¨æ˜ï¼Œä»…å‡­é—®é¢˜çš„æ¿€æ´»ä¿¡æ¯å³å¯é¢„æµ‹æ¨ç†tokençš„æ•°é‡ï¼Œè¡¨æ˜æ¨¡å‹èƒ½å¤Ÿæå‰ä¼°è®¡æ‰€éœ€çš„æ¨ç†å¼ºåº¦ã€‚è¿›ä¸€æ­¥åˆ†æå‘ç°ï¼ŒLRMsé€šè¿‡åµŒå…¥åœ¨æ¿€æ´»ä¸­çš„é¢„åˆ†é…æ–¹å‘å‘é‡ç¼–ç è¿™ä¸€æ¨ç†å¼ºåº¦ï¼Œè¯¥å‘é‡çš„å¤§å°è°ƒèŠ‚æ¨ç†å¼ºåº¦ã€‚å¯¹è¯¥å‘é‡çš„å‡æ³•æ“ä½œä¼šå¯¼è‡´æ¨ç†tokenæ•°å’Œæ€§èƒ½ä¸‹é™ï¼Œè€ŒåŠ æ³•æ“ä½œåˆ™ä¼šå¢åŠ æ¨ç†tokenæ•°å¹¶æå‡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹å‘å‘é‡æŒç»­å¯¹æ¨ç†é•¿åº¦äº§ç”Ÿæ­£å‘é¢„æµ‹æ•ˆæœï¼Œå¹¶é€šè¿‡ä¿®æ”¹ç»ˆæ­¢æ¨ç†tokenï¼ˆ</think>ï¼‰çš„logitsæ¥å½±å“æ¨ç†è¿‡ç¨‹çš„é•¿åº¦ã€‚ |
 </details>                                             

#### 

<details>
<summary> CoT-Valve: Length-Compressible Chain-of-Thought Tuning<a href="https://arxiv.org/pdf/2502.09601" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **Length-Compressible Chain-of-Thought (CoT) Tuning**, a fine-tuning strategy that learns a directional vector in parameter space to effectively control the length of reasoning chains. | æœ¬æ–‡æå‡ºä¸€ç§**é•¿åº¦å¯å‹ç¼©çš„Chain-of-Thoughtå¾®è°ƒç­–ç•¥**ï¼ˆLength-Compressible CoT Tuningï¼‰ï¼Œé€šè¿‡åœ¨å‚æ•°ç©ºé—´ä¸­å­¦ä¹ ä¸€æ¡æ–¹å‘å‘é‡ï¼Œå®ç°å¯¹æ¨ç†é“¾é•¿åº¦çš„æœ‰æ•ˆæ§åˆ¶ã€‚ |
 </details>                                                 

#### 

<details>
<summary>Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs <a href="https://arxiv.org/pdf/2506.07240" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper finds that large language models implicitly monitor their relative reasoning progress during explicit reasoning stages. Based on this insight, it proposes the **Thinking Progress Vector** method, which dynamically regulates reasoning length by intervening in this vector, thereby alleviating the issue of overthinking. | æœ¬æ–‡å‘ç°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ˜¾å¼æ¨ç†é˜¶æ®µèƒ½å¤Ÿå†…éšåœ°ç›‘æ§è‡ªèº«çš„ç›¸å¯¹æ€ç»´è¿›åº¦ï¼Œæ®æ­¤æå‡ºâ€œæ€ç»´è¿›åº¦å‘é‡â€æ–¹æ³•ï¼Œé€šè¿‡å¹²é¢„è¯¥å‘é‡å®ç°å¯¹æ¨ç†é•¿åº¦çš„åŠ¨æ€è°ƒæ§ï¼Œä»è€Œç¼“è§£è¿‡åº¦æ¨ç†é—®é¢˜ã€‚ |
 </details>                                                  

#### 

<details>
<summary>Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute <a href="https://arxiv.org/pdf/2506.15882" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| To enable fine-grained control over the reasoning behavior of large language models, **Fractional Reasoning** introduces a latent-space-guided method for reasoning-phase modulation. This approach constructs an implicit offset vector by contrasting positive and negative prompts, and injects it into the modelâ€™s hidden states with adjustable intensity during inference. Unlike fixed instruction-based prompting, this method requires no additional training, is model-agnostic, and supports adaptive reasoning control across tasks with varying complexity. It demonstrates superior performance and enhanced interpretability in both breadth-oriented and depth-oriented reasoning scenarios. | ä¸ºå®ç°å¯¹å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¡Œä¸ºçš„ç²¾ç»†è°ƒæ§ï¼ŒFractional Reasoning æå‡ºäº†ä¸€ç§åŸºäºéšç©ºé—´å¼•å¯¼å‘é‡çš„æ¨ç†é˜¶æ®µæ§åˆ¶æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”æ­£è´Ÿæç¤ºç”Ÿæˆéšå¼åç§»å‘é‡ï¼Œå¹¶åœ¨æ¨ç†æ—¶ä»¥å¯è°ƒå¼ºåº¦æ³¨å…¥æ¨¡å‹éšçŠ¶æ€ï¼Œä»è€Œå®ç°å¯¹æç¤ºå¼ºåº¦çš„è¿ç»­æ§åˆ¶ã€‚ç›¸æ¯”å›ºå®šæŒ‡ä»¤æç¤ºï¼Œè¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒï¼Œæ¨¡å‹æ— å…³ï¼Œé€‚ç”¨äºä¸åŒä»»åŠ¡å¤æ‚åº¦ä¸‹çš„è‡ªé€‚åº”æ¨ç†è°ƒèŠ‚ï¼Œåœ¨å¹¿åº¦å‹ä¸æ·±åº¦å‹æ¨ç†ç­–ç•¥ä¸­å‡å±•ç°å‡ºæ›´ä¼˜æ€§èƒ½ä¸æ›´å¼ºå¯è§£é‡Šæ€§ã€‚ |
</details>                                                 

#### 

<details>
<summary>Mitigating Overthinking in Large Reasoning Models via Manifold Steering <a href="https://arxiv.org/pdf/2505.22411" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Through mechanistic interpretability analysis, the authors find that the phenomenon of overthinking can be characterized by a specific low-dimensional manifold within the modelâ€™s activation space. They propose an intervention method based on this manifold, called **Manifold Steering**. By projecting interventions onto the low-dimensional manifold, this method effectively reduces interference from high-dimensional noise, mitigating the computational overhead and performance degradation caused by overthinking. | ä½œè€…é€šè¿‡æœºåˆ¶å¯è§£é‡Šæ€§åˆ†æå‘ç°ï¼Œè¿‡åº¦æ€è€ƒç°è±¡å¯ä»¥ç”¨æ¨¡å‹æ¿€æ´»ç©ºé—´ä¸­çš„ç‰¹å®šä½ç»´æµå½¢è¡¨ç¤ºï¼Œå¹¶æå‡ºäº†åŸºäºè¯¥æµå½¢çš„å¹²é¢„æ–¹æ³•â€”â€”æµå½¢å¼•å¯¼ï¼ˆManifold Steeringï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¹²é¢„æŠ•å½±åˆ°ä½ç»´æµå½¢ä¸Šï¼Œæœ‰æ•ˆå‡å°‘é«˜ç»´å™ªå£°çš„å¹²æ‰°ï¼Œç¼“è§£äº†è¿‡åº¦æ€è€ƒå¸¦æ¥çš„è®¡ç®—å¼€é”€å’Œæ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ |
 </details>                                                 

## ğŸŒŸ Efficient Reasoning with Model Collaboration
Efficient reasoning with model collaboration aims to enhance reasoning efficiency and accuracy in LLMs by enabling cooperation between multiple LLMs, each leveraging distinct reasoning strengths. Unlike single model efficient reasoning method, collaborative frameworks strategically combine long-chain reasoning models (long CoT) that excel at handling complex tasks and short-chain reasoning models (short CoT) that are lightweight and efficient for general tasks. This synergy allows for more fine-grained and cost-effective control of the reasoning process. Specific methods include Longâ€“Short Model Collaboration, LLM Routing, Model Consolidation, and Speculative Decoding.

### Longâ€“Short Model Collaboration

<details>
<summary> SplitReason: Learning To Offload Reasoning<a href="https://arxiv.org/pdf/2504.16379" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This method assigns the main reasoning tasks to the small model while dynamically offloading complex reasoning steps that the small model cannot handle to the large model, enabling collaborative reasoning between the two. The training process follows a two-stage strategy: first, supervised fine-tuning (SFT) with control tokens (denoted by special tags `<bigmodel>...</bigmodel>`) teaches the small model to insert offloading boundaries; second, reinforcement learning (GRPO) optimizes the offloading behavior using a composite reward based on accuracy, formatting compliance, and offloading ratio, balancing inference speed and performance. | è¯¥æ–¹æ³•è®©å°æ¨¡å‹è´Ÿè´£ä¸»è¦æ¨ç†ï¼ŒåŠ¨æ€å°†å°æ¨¡å‹è‡ªèº«æ— æ³•å¤„ç†çš„å¤æ‚æ¨ç†æ­¥éª¤å¸è½½ç»™å¤§æ¨¡å‹å¤„ç†ï¼Œå®ç°å°æ¨¡å‹ä¸å¤§æ¨¡å‹çš„ååŒæ¨ç†ã€‚è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼šå…ˆé€šè¿‡å¸¦æœ‰æ§åˆ¶æ ‡è®°çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½¿å°æ¨¡å‹å­¦ä¼šæ’å…¥å¸è½½è¾¹ç•Œï¼ˆç”±ç‰¹æ®Šæ§åˆ¶æ ‡è®°<bigmodel>...</bigmodel>æ„æˆï¼‰ï¼Œå†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆGRPOï¼‰åŸºäºå‡†ç¡®æ€§ã€æ ¼å¼è§„èŒƒå’Œå¸è½½æ¯”ä¾‹è®¾è®¡ç»¼åˆå¥–åŠ±ï¼Œä¼˜åŒ–å¸è½½è¡Œä¸ºä»¥å¹³è¡¡æ¨ç†é€Ÿåº¦å’Œæ€§èƒ½ã€‚ |
</details>

#### 

<details>
<summary>Thought manipulation: External thought can be efficient for large reasoning models <a href="https://arxiv.org/pdf/2504.13626" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **ThoughtMani**, a collaborative reasoning framework that integrates small and large language models. In the reasoning phase, ThoughtMani employs a small model to generate high-level chain-of-thought (CoT) reasoning, which is then appended as a prompt to the input of a large language model, guiding it toward more efficient inference. | æå‡ºäº†ä¸€ç§èåˆå°æ¨¡å‹ä¸å¤§æ¨¡å‹çš„ååŒæ¨ç†æ¡†æ¶ThoughtManiï¼ŒThoughtManiä½¿ç”¨å°æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µç”Ÿæˆé«˜å±‚æ¬¡æ¨ç†é“¾ï¼ˆCoTï¼‰ï¼Œå¹¶å°†å…¶ä½œä¸ºæç¤ºæ‹¼æ¥åˆ°å¤§æ¨¡å‹è¾“å…¥ä¸­ï¼Œå¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ›´åŠ é«˜æ•ˆçš„æ¨ç†ã€‚ |
</details>

#### 

<details>
<summary>CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models <a href="https://arxiv.org/pdf/2505.22017" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| The authors propose a two-stage framework, **CoThink**. First, an instruction model generates a high-level solution outline for the problem, which is then used to guide a large reasoning model to perform detailed inference. | ä½œè€…æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶CoThinkï¼Œé¦–å…ˆç”±æŒ‡ä»¤æ¨¡å‹ç”Ÿæˆé—®é¢˜çš„é«˜å±‚æ¬¡è§£å†³æ–¹æ¡ˆå¤§çº²ï¼Œéšåæ ¹æ®è¯¥å¤§çº²å¼•å¯¼æ¨ç†å¤§æ¨¡å‹è¿›è¡Œè¯¦ç»†æ¨ç†ã€‚ |
</details>

#### 

<details>
<summary> Plan and Budget: Effective and Efficient Test-Time Scaling on Large Language Model Reasoning<a href="https://arxiv.org/abs/2505.16122" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| The **PLAN-AND-BUDGET** method proposes a reasoning framework that **dynamically allocates inference budgets based on task structure and uncertainty**. The framework consists of two stages: first, a lightweight model automatically decomposes the original question into several subproblems, and estimates the complexity of each subproblem using indicators such as model confidence. Then, token budgets are normalized and dynamically allocated to each subproblem based on their estimated complexity. During inference, each subproblem is solved within its assigned budget, and a final answer is produced by an aggregation module. | PLAN-AND-BUDGETæ–¹æ³•æå‡ºäº†ä¸€ç§**åŸºäºä»»åŠ¡ç»“æ„ä¸ä¸ç¡®å®šæ€§åŠ¨æ€åˆ†é…æ¨ç†é¢„ç®—**çš„æ¨ç†æ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨è½»é‡æ¨¡å‹å°†åŸå§‹é—®é¢˜è‡ªåŠ¨åˆ†è§£ä¸ºè‹¥å¹²å­é—®é¢˜ï¼Œå¹¶ç»“åˆæ¨¡å‹ç½®ä¿¡åº¦ç­‰æŒ‡æ ‡ä¼°è®¡æ¯ä¸ªå­é—®é¢˜çš„å¤æ‚åº¦ï¼›éšåï¼Œæ ¹æ®å¤æ‚åº¦è¿›è¡Œå½’ä¸€åŒ–åˆ†é…ï¼ŒåŠ¨æ€åˆ†é…tokené¢„ç®—è‡³å„å­é—®é¢˜ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªå­é—®é¢˜åœ¨æŒ‡å®šé¢„ç®—ä¸‹å®Œæˆç”Ÿæˆï¼Œå¹¶ç”±èšåˆæ¨¡å—æ•´åˆæœ€ç»ˆç­”æ¡ˆã€‚ |
</details>

#### 

<details>
<summary>VeriThinker: Learning to Verify Makes Reasoning Model Efficient <a href="https://arxiv.org/pdf/2505.17941" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| VeriThinker proposes a method called Supervised Verification Fine-Tuning (SVFT), which aims to enhance the modelâ€™s ability to assess the correctness of chain-of-thought (CoT) answers. Based on this method, when given a question, a large language model (LLM) using short-chain reasoning first generates a candidate answer and verifies its correctness; if deemed correct, the system outputs the result directly, otherwise it triggers a long-chain reasoning process to produce a more detailed answer. | VeriThinkeræå‡ºäº†ä¸€ç§ç›‘ç£éªŒè¯å¾®è°ƒï¼ˆSupervised Verification Fine-Tuning, SVFTï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨æå‡æ¨¡å‹å¯¹é“¾å¼æ¨ç†ï¼ˆCoTï¼‰è§£ç­”æ­£ç¡®æ€§çš„åˆ¤åˆ«èƒ½åŠ›ã€‚åŸºäºæ­¤æ–¹æ³•ï¼Œé¢å¯¹ä¸€ä¸ªé—®é¢˜æ—¶ï¼ŒçŸ­é“¾æ¨ç†çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¦–å…ˆç”Ÿæˆä¸€ä¸ªè§£ç­”å€™é€‰ï¼Œå¹¶å¯¹å…¶æ­£ç¡®æ€§è¿›è¡ŒéªŒè¯ï¼›è‹¥åˆ¤å®šä¸ºæ­£ç¡®ï¼Œç³»ç»Ÿç›´æ¥è¾“å‡ºè¯¥ç»“æœï¼Œè‹¥åˆ¤å®šä¸ºä¸æ­£ç¡®ï¼Œåˆ™å¯åŠ¨é•¿é“¾æ¨ç†è¿‡ç¨‹ä»¥ç”Ÿæˆæ›´è¯¦ç»†çš„è§£ç­”ã€‚ |
</details>

#### 

<details>
<summary> Guiding Reasoning in Small Language Models with LLM Assistance<a href="https://arxiv.org/pdf/2504.09923" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes the **SMART** framework, where a small language model (SLM) first generates an initial reasoning draft. Then, a scoring mechanism is used to evaluate each step of the reasoning process in real time, identifying uncertain or potentially erroneous steps. The framework selectively calls a large language model (LLM) for corrections, after which the SLM continues reasoning along the corrected path, significantly reducing computational costs while ensuring reasoning accuracy. | è¯¥è®ºæ–‡æå‡ºSMARTæ¡†æ¶ï¼Œé¦–å…ˆç”±å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ç”Ÿæˆåˆå§‹æ¨ç†è‰ç¨¿ï¼Œç„¶åé€šè¿‡æ‰“åˆ†æœºåˆ¶å¯¹æ¯ä¸€æ­¥æ¨ç†è¿‡ç¨‹è¿›è¡Œå®æ—¶è¯„ä¼°ï¼Œè¯†åˆ«å‡ºä¸ç¡®å®šæˆ–å¯èƒ½é”™è¯¯çš„æ­¥éª¤ï¼Œå¹¶é€‰æ‹©æ€§åœ°è°ƒç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä¿®æ­£ï¼Œæœ€ç»ˆç”±SLMæ²¿ç€ä¿®æ­£åçš„è·¯å¾„å®Œæˆæ¨ç†ï¼Œä»è€Œåœ¨ä¿è¯æ¨ç†å‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚ |
</details>

#### 

<details>
<summary>What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding <a href="https://arxiv.org/pdf/2506.06998" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **FoReaL-Decoding**, a two-stage decoding method. A strong **leading model** first generates the initial tokens of each sentence to steer the reasoning style, after which a lightweight **draft model** completes the remaining content. To prevent excessive intervention by the leading model, FoReaL-Decoding introduces a **stochastic gating mechanism** that controls the frequency of the leading modelâ€™s involvement. | æœ¬æ–‡æå‡º FoReaL-Decoding æ–¹æ³•ï¼šé¦–å…ˆç”±ä¸€ä¸ªå¼ºå¤§çš„å¼•å¯¼æ¨¡å‹ï¼ˆLeading modelï¼‰ç”Ÿæˆæ¯ä¸ªå¥å­çš„å‰è‹¥å¹²ä¸ª tokenï¼Œä»¥å¼•å¯¼æ¨ç†é£æ ¼ï¼›éšåç”±ä¸€ä¸ªè½»é‡çš„è‰ç¨¿æ¨¡å‹ï¼ˆDraft modelï¼‰å®Œæˆå‰©ä½™å†…å®¹ã€‚ä¸ºé¿å…å¼•å¯¼æ¨¡å‹çš„è¿‡åº¦ä»‹å…¥ï¼ŒFoReaL-Decoding è¿˜è®¾è®¡äº†éšæœºé—¨æ§æœºåˆ¶ï¼Œç”¨äºæ§åˆ¶å¼•å¯¼æ¨¡å‹çš„ä»‹å…¥é¢‘ç‡ã€‚ |
</details>

#### 

<details>
<summary> Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning<a href="https://arxiv.org/pdf/2505.11827" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| The authors propose the **LongâŠ—Short** framework, which first fine-tunes a large language model on synthetic data instructions to enable both long-form and short-form reasoning capabilities. Building on this, they design a multi-turn dialogue-based reinforcement learning method to achieve collaborative reasoning between the long- and short-form reasoning models. By combining rewards based on the final answer accuracy, format, and length, the method encourages the long-form reasoning model to focus on generating key reasoning steps, while the short-form model produces the remaining concise reasoning content, thereby improving reasoning efficiency and effectiveness. | ä½œè€…æå‡ºäº†LongâŠ—Shortæ¡†æ¶ï¼Œé¦–å…ˆé€šè¿‡åˆæˆæ•°æ®æŒ‡ä»¤å¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåˆ†åˆ«å¾—åˆ°å…·å¤‡é•¿æ€ç»´æ¨ç†å’ŒçŸ­æ€ç»´æ¨ç†çš„èƒ½åŠ›çš„æ¨¡å‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºå¤šè½®å¯¹è¯çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ç°é•¿æ€ç»´ä¸çŸ­æ€ç»´æ¨¡å‹çš„ååŒæ¨ç†ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆæœ€ç»ˆç­”æ¡ˆçš„å¥–åŠ±ä»¥åŠæ ¼å¼å’Œé•¿åº¦çš„å¥–åŠ±ï¼Œä¿ƒè¿›é•¿æ€ç»´æ¨¡å‹ä¸“æ³¨ç”Ÿæˆå…³é”®æ¨ç†æ­¥éª¤ï¼Œè€ŒçŸ­æ€ç»´æ¨¡å‹åˆ™è´Ÿè´£ç”Ÿæˆå‰©ä½™çš„ç®€æ´æ¨ç†å†…å®¹ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ä¸æ•ˆæœã€‚ |
</details>



<details>
<summary> Collaborative LLM Inference via Planning for Efficient Reasoning<a href="https://arxiv.org/pdf/2506.11578" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **COPE**, a multi-turn reasoning framework featuring alternating collaboration between a small model and a large model. COPE generates plans in stages to guide the reasoning process and employs a sampling-and-voting consensus mechanism for answers at each turn. If consensus is not reached, the question is progressively escalated to a more powerful model. Through multi-turn interactions and plan reuse, this method improves both reasoning efficiency and accuracy. | è¿™ç¯‡è®ºæ–‡æå‡ºäº†COPEï¼Œä¸€ç§å°æ¨¡å‹ä¸å¤§æ¨¡å‹äº¤æ›¿åä½œçš„å¤šè½®æ¨ç†æ¡†æ¶ã€‚COPEé€šè¿‡åˆ†é˜¶æ®µç”Ÿæˆè§„åˆ’å¹¶å¼•å¯¼æ¨ç†è¿‡ç¨‹ï¼Œå®ç°äº†åœ¨æ¯è½®åŸºäºé‡‡æ ·ä¸æŠ•ç¥¨çš„ç­”æ¡ˆå…±è¯†æœºåˆ¶ï¼Œæœªè¾¾å…±è¯†æ—¶é€æ­¥å°†é—®é¢˜å‡çº§è‡³æ›´å¼ºæ¨¡å‹å¤„ç†ã€‚è¯¥æ–¹æ³•é€šè¿‡å¤šè½®äº¤äº’å’Œè§„åˆ’å¤ç”¨ï¼Œæå‡äº†æ¨ç†çš„æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚ |
 </details> 


<details>
<summary> ThinkSwitcher: When to Think Hard, When to Think Fast<a href="https://arxiv.org/pdf/2505.14183" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Unlike other approaches that rely on reinforcement learning for adaptive reasoning, **ThinkSwitcher** introduces a lightweight reasoning mode switcher that dynamically selects between short- and long-chain reasoning paths based on input complexity, without requiring additional training of the reasoning model itself. The switcher takes the representation vector of the input question and predicts the expected performance of both reasoning modes. During training, ThinkSwitcher adopts a multi-sampling evaluation strategy by generating multiple responses for each mode and constructs a continuous supervision signal based on empirical success rates, thereby avoiding instability caused by binary classification labels. At inference time, the model selects the optimal reasoning path according to the switcher's prediction, effectively balancing performance and efficiency. | ä¸åŒäºå…¶ä»–æ–¹æ³•é‡‡ç”¨å¼ºåŒ–å­¦ä¹ å®ç°è‡ªé€‚åº”å­¦ä¹ ï¼Œ**ThinkSwitcher** é€šè¿‡æ„å»ºè½»é‡çº§çš„æ¨¡å¼åˆ‡æ¢å™¨ï¼Œåœ¨æ— éœ€é¢å¤–è®­ç»ƒæ¨ç†æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œå®ç°äº†åŸºäºè¾“å…¥å¤æ‚åº¦çš„æ¨ç†æ¨¡å¼åŠ¨æ€é€‰æ‹©ã€‚è¯¥åˆ‡æ¢å™¨ä»¥é—®é¢˜çš„è¡¨ç¤ºå‘é‡ä¸ºè¾“å…¥ï¼Œé¢„æµ‹çŸ­é“¾ä¸é•¿é“¾æ€ç»´è·¯å¾„çš„æ¨ç†è¡¨ç°ã€‚è®­ç»ƒé˜¶æ®µï¼ŒThinkSwitcher é‡‡ç”¨å¤šæ¬¡é‡‡æ ·è¯„ä¼°ç­–ç•¥ï¼Œä¸ºæ¯ä¸ªæ¨ç†æ¨¡å¼ç”Ÿæˆå¤šä¸ªå“åº”ï¼Œå¹¶åŸºäºç»éªŒé€šè¿‡ç‡æ„å»ºè¿ç»­å‹ç›‘ç£ä¿¡å·ï¼Œä»è€Œé¿å…äº†äºŒåˆ†ç±»æ ‡ç­¾å¸¦æ¥çš„ä¸ç¨³å®šæ€§ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹ä¾æ®åˆ‡æ¢å™¨çš„åˆ¤æ–­é€‰æ‹©æœ€ä¼˜æ¨ç†è·¯å¾„ï¼Œä»è€Œå®ç°æ€§èƒ½ä¸æ•ˆç‡çš„æœ‰æ•ˆååŒã€‚ |
</details> 

### LLM Routing

<details>
<summary>Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning <a href="https://arxiv.org/pdf/2505.20664" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper first constructs a dataset annotated with whether reasoning is required based on question difficulty, and uses it to train a reasoning mode router. During routing, a lightweight pre-reasoning stage is introduced to extract **capability-aware embeddings** from the model's hidden layers, which are used to assess the model's ability to solve the current problem in real time. If the assessment indicates that the problem is complex, the reasoning mode is triggered to generate a detailed reasoning chain; otherwise, a general mode is adopted to directly produce the answer, thereby avoiding excessive reasoning for simple questions. | æœ¬æ–‡é¦–å…ˆåŸºäºé—®é¢˜éš¾åº¦æ„å»ºäº†ä¸€ä¸ªæ ‡æ³¨æ˜¯å¦éœ€è¦æ¨ç†çš„æ•°æ®é›†ï¼Œå¹¶æ®æ­¤è®­ç»ƒæ¨ç†æ¨¡å¼è·¯ç”±å™¨ã€‚åœ¨è¿›è¡Œè·¯ç”±æ—¶ï¼Œå¼•å…¥è½»é‡çº§çš„é¢„æ¨ç†é˜¶æ®µï¼Œä»æ¨¡å‹çš„éšè—å±‚ä¸­æå–èƒ½åŠ›æ„ŸçŸ¥åµŒå…¥ï¼ˆCapability-aware Embeddingsï¼‰ï¼Œç”¨äºå®æ—¶è¯„ä¼°æ¨¡å‹è§£å†³å½“å‰é—®é¢˜çš„èƒ½åŠ›ã€‚è‹¥è¯„ä¼°ç»“æœè¡¨æ˜é—®é¢˜è¾ƒä¸ºå¤æ‚ï¼Œåˆ™è§¦å‘æ¨ç†æ¨¡å¼ä»¥ç”Ÿæˆè¯¦ç»†çš„æ¨ç†é“¾ï¼›è‹¥é—®é¢˜è¾ƒä¸ºç®€å•ï¼Œåˆ™ç›´æ¥é‡‡ç”¨é€šç”¨æ¨¡å¼ç”Ÿæˆç­”æ¡ˆï¼Œä»è€Œé¿å…å¯¹ç®€å•é—®é¢˜çš„è¿‡åº¦æ¨ç†ã€‚ |
</details>

#### 

<details>
<summary>Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced Model Router <a href="https://arxiv.org/pdf/2506.05901" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **R2-Reasoner**, a general framework for multi-model collaborative reasoning based on reinforcement learning. The framework consists of a task decomposer and a subtask allocator: the former breaks down complex tasks into a sequence of well-structured and logically ordered subtasks, while the latter assigns each subtask to the most suitable model within a heterogeneous model pool based on difficulty. During training, both modules are first supervised-finetuned using a constructed dataset, followed by staged reinforcement learning that alternately optimizes their parameters, enabling efficient and adaptive reasoning routing. | æœ¬æ–‡æå‡º **R2-Reasoner**ï¼Œä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°å¤šæ¨¡å‹ååŒæ¨ç†çš„é€šç”¨æ¡†æ¶ã€‚è¯¥æ–¹æ³•ç”±ä»»åŠ¡åˆ†è§£å™¨å’Œå­ä»»åŠ¡åˆ†é…å™¨ç»„æˆï¼šå‰è€…å°†å¤æ‚ä»»åŠ¡æ‹†è§£ä¸ºç»“æ„æ¸…æ™°ã€é€»è¾‘æœ‰åºçš„å­ä»»åŠ¡åºåˆ—ï¼Œåè€…åˆ™æ ¹æ®å­ä»»åŠ¡éš¾åº¦ï¼Œå°†å…¶åˆ†é…ç»™å¼‚æ„æ¨¡å‹æ± ä¸­æœ€åˆé€‚çš„æ¨¡å‹ã€‚è®­ç»ƒé˜¶æ®µå…ˆåˆ©ç”¨æ„é€ çš„æ•°æ®é›†å¯¹ä¸¤ä¸ªæ¨¡å—è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œéšåé€šè¿‡åˆ†é˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ è½®æµä¼˜åŒ–ä¸¤æ¨¡å—å‚æ•°ï¼Œå®ç°é«˜æ•ˆã€è‡ªé€‚åº”çš„æ¨ç†è·¯ç”±ã€‚ |
</details>

#### 

<details>
<summary> TagRouter: Learning Route to LLMs through Tags for Open-Domain Text Generation Tasks<a href="https://www.arxiv.org/abs/2506.12473" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **TagRouter**, a training-free model routing method for open-domain text generation tasks. It enables collaborative reasoning and cost control across multiple large language models by introducing **self-aware tags**. The TagRouter framework consists of three modules: the **TagGenerator**, which produces semantically relevant tags for the input query; the **TagScorer**, which builds a mapping between tags and candidate model performance based on existing data; and the **TagDecider**, which selects and routes models automatically based on predefined optimization strategies, enabling efficient and parameter-free reasoning. | è¯¥è®ºæ–‡æå‡º **TagRouter**ï¼Œä¸€ç§é¢å‘å¼€åŸŸæ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„**è®­ç»ƒæ— å…³æ¨¡å‹è·¯ç”±æ–¹æ³•**ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥**è‡ªæ„ŸçŸ¥æ ‡ç­¾**å®ç°å¤šå¤§è¯­è¨€æ¨¡å‹ä¹‹é—´çš„ååŒæ¨ç†ä¸æˆæœ¬æ§åˆ¶ã€‚TAGROUTER æ¡†æ¶ç”±ä¸‰ä¸ªæ¨¡å—ç»„æˆï¼š**Tag Generator** ç”¨äºä¸ºè¾“å…¥æŸ¥è¯¢ç”Ÿæˆè¯­ä¹‰ç›¸å…³çš„æ ‡ç­¾ï¼Œ**TagScorer** åŸºäºå·²æœ‰æ•°æ®å»ºç«‹æ ‡ç­¾ä¸å€™é€‰æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œ**TagDecider** åˆ™æ ¹æ®è®¾å®šçš„ä¼˜åŒ–ç­–ç•¥å®ç°è‡ªåŠ¨åŒ–ã€æ— éœ€è°ƒå‚çš„æ¨¡å‹é€‰æ‹©ä¸è·¯ç”±ï¼Œå®ç°é«˜æ•ˆæ¨ç†ã€‚ |
</details>

#### 

<details>
<summary> Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models<a href="https://arxiv.org/pdf/2506.04182" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| **SwitchCoT** proposes a dynamic instance-based strategy for switching between long-chain and short-chain reasoning. A lightweight policy selector chooses the most appropriate reasoning approach based on input features and computational budget, while prompt engineering controls the language model's generation process. This achieves a balance between efficiency and accuracy without modifying the model parameters. | SwitchCoT æå‡ºäº†ä¸€ç§åŸºäºå®ä¾‹åŠ¨æ€åˆ‡æ¢é•¿çŸ­é“¾å¼æ¨ç†ç­–ç•¥ï¼Œé€šè¿‡è½»é‡çº§çš„ç­–ç•¥é€‰æ‹©å™¨æ ¹æ®è¾“å…¥ç‰¹å¾ä¸è®¡ç®—é¢„ç®—é€‰æ‹©æœ€åˆé€‚çš„æ¨ç†æ–¹å¼ï¼Œå¹¶ç»“åˆæç¤ºå·¥ç¨‹æ§åˆ¶è¯­è¨€æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ï¼Œåœ¨ä¸ä¿®æ”¹æ¨¡å‹å‚æ•°çš„å‰æä¸‹å®ç°é«˜æ•ˆä¸å‡†ç¡®æ€§çš„å¹³è¡¡ã€‚ |
</details>

#### 

<details>
<summary>Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning <a href="https://arxiv.org/pdf/2506.09033" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Router-R1** models the routing process as a sequential decision-making task, where the router itself is a reasoning-capable language model. It dynamically alternates between â€œthinkingâ€ and â€œinvokingâ€ actions to enable adaptive collaboration among multiple models during task execution. A regularized reward functionâ€”comprising format consistency, task accuracy, and computational costâ€”guides the model to balance performance and efficiency. | Router-R1 æ˜¯é€šè¿‡å°†è·¯ç”±è¿‡ç¨‹å»ºæ¨¡ä¸ºåºè´¯å†³ç­–ä»»åŠ¡ï¼Œå¹¶å°†è·¯ç”±å™¨æœ¬èº«è®¾è®¡ä¸ºå…·å¤‡æ¨ç†èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹ï¼Œå®ç°â€œæ€è€ƒâ€ä¸â€œè°ƒç”¨â€æ“ä½œçš„åŠ¨æ€äº¤æ›¿ï¼Œä»è€Œåœ¨ä»»åŠ¡æ‰§è¡Œä¸­è‡ªé€‚åº”åœ°ååŒå¤šä¸ªæ¨¡å‹ã€‚å…¶é€šè¿‡ç”±æ ¼å¼ã€ç»“æœä¸æˆæœ¬æ„æˆçš„è§„åˆ™åŒ–å¥–åŠ±å‡½æ•°ï¼Œå¼•å¯¼æ¨¡å‹åœ¨æ€§èƒ½ä¸å¼€é”€ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ |
</details>

#### 

<details>
<summary>R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing <a href="https://arxiv.org/pdf/2505.21600" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **Roads to Rome (R2R)**, a token-level routing approach that enables efficient and high-quality reasoning with small language models (SLMs) by invoking large language models (LLMs) only for key tokens that may lead to reasoning divergence. By combining automatic annotation with a lightweight router, R2R significantly improves inference efficiency while maintaining the accuracy of the reasoning process. | è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œRoads to Rome (R2R)â€çš„é€è¯è·¯ç”±æ–¹æ³•ï¼Œé¦–å…ˆé€šè¿‡å°æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œä»…å¯¹æ¨ç†ä¸­ä¼šå¯¼è‡´è·¯å¾„åˆ†æ­§çš„å…³é”®è¯è°ƒç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªåŠ¨æ ‡æ³¨å’Œè½»é‡è·¯ç”±å™¨ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†æ¨ç†ç»“æœçš„å‡†ç¡®æ€§ã€‚ |
</details>

### Model Consolidation

<details>
<summary>Thinking without Tokens by Habitual Reasoning Distillation with Multi-Teachersâ€™ Guidance <a href="https://arxiv.org/pdf/2503.24198" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| **TwT** first proposes a reasoning path synthesis framework that leverages multiple teachers to collaboratively generate multiple candidate reasoning paths. These paths are then filtered using quality and diversity metrics to form a high-quality reasoning path dataset. Building on this, a habitual reasoning distillation method is introduced, consisting of three stages that progressively compress and internalize the teachersâ€™ full reasoning capabilities into the student model. In the first stage, the student learns the complete reasoning paths from the teachers. In the second stage, the teachers revise and compress the reasoning paths based on the studentâ€™s performance, creating a new dataset for further student training. In the third stage, the student trains solely on the final answers, developing the ability to perform tasks efficiently without relying on explicit reasoning paths. | TwTé¦–å…ˆæå‡ºæ¨ç†è·¯å¾„åˆæˆæ¡†æ¶ï¼Œåˆ©ç”¨å¤šä¸ªæ•™å¸ˆååŒç”Ÿæˆå¤šæ¡å€™é€‰æ¨ç†è·¯å¾„ï¼Œä¹‹åé€šè¿‡è´¨é‡ä¸å¤šæ ·æ€§ç­‰æ£€æµ‹æŒ‡æ ‡ï¼Œç­›é€‰å¹¶å½¢æˆé«˜è´¨é‡çš„æ¨ç†è·¯å¾„æ•°æ®é›†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†ä¹ æƒ¯æ€§æ¨ç†è’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”±3ä¸ªé˜¶æ®µç»„æˆï¼Œå°†æ•™å¸ˆæ¨¡å‹çš„å®Œæ•´æ¨ç†èƒ½åŠ›é€æ­¥å‹ç¼©å¹¶å†…åŒ–åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œå­¦ç”Ÿå­¦ä¹ æ•™å¸ˆçš„å®Œæ•´æ¨ç†è·¯å¾„ï¼›ç¬¬äºŒé˜¶æ®µï¼Œæ•™å¸ˆæ ¹æ®å­¦ç”Ÿçš„è¡¨ç°ï¼Œä¿®æ”¹å¹¶å‹ç¼©æ¨ç†è·¯å¾„ï¼Œå½¢æˆæ–°çš„æ•°æ®é›†ä¾›å­¦ç”Ÿå­¦ä¹ ï¼›ç¬¬ä¸‰é˜¶æ®µï¼Œå­¦ç”Ÿä»…ä¾èµ–æœ€ç»ˆç­”æ¡ˆè¿›è¡Œè®­ç»ƒï¼Œå½¢æˆæ— éœ€æ˜¾å¼æ¨ç†å³å¯é«˜æ•ˆå®Œæˆä»»åŠ¡çš„èƒ½åŠ›ã€‚ |
</details>

#### 

<details>
<summary> DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models<a href="https://arxiv.org/pdf/2505.13975" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This method prunes and optimizes the student modelâ€™s initial chain-of-thought by leveraging teacher models based on skill decomposition. Redundant steps are merged and irrelevant parts are removed to produce concise and semantically coherent reasoning units, which form the dataset. The student model is trained via supervised fine-tuning on this dataset, enabling efficient reasoning. | è¯¥æ–¹æ³•é€šè¿‡æ•™å¸ˆæ¨¡å‹åŸºäºæŠ€èƒ½åˆ†è§£å¯¹å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆçš„åˆå§‹é“¾å¼æ¨ç†è½¨è¿¹è¿›è¡Œå‰ªæå’Œä¼˜åŒ–ï¼Œåˆå¹¶å†—ä½™æ­¥éª¤å¹¶åˆ é™¤æ— å…³éƒ¨åˆ†ï¼Œä»è€Œç”Ÿæˆç®€æ´ä¸”è¯­ä¹‰è¿è´¯çš„æ¨ç†å•å…ƒï¼Œå½¢æˆæ•°æ®é›†ã€‚å­¦ç”Ÿæ¨¡å‹åˆ©ç”¨ç›‘ç£å¾®è°ƒçš„æ–¹å¼è¿›è¡Œå­¦ä¹ ï¼Œä»è€Œå®ç°é«˜æ•ˆæ¨ç†ã€‚ |
</details>

#### 

<details>
<summary> Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting<a href="https://arxiv.org/pdf/2505.19716" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes a concise and efficient reasoning data distillation methodâ€”**Difficulty-Aware Prompting (DAP)**. It leverages a teacher model (e.g., DeepSeek-R1) to rewrite reasoning chains based on question difficulty, generating adaptive and concise reasoning paths. A new dataset, **LiteCoT**, containing 100,000 samples, is constructed for model fine-tuning. | æœ¬æ–‡æå‡ºä¸€ç§ç®€æ´é«˜æ•ˆçš„æ¨ç†æ•°æ®è’¸é¦æ–¹æ³•â€”â€”éš¾åº¦æ„ŸçŸ¥æç¤ºï¼ˆDAPï¼‰ï¼Œåˆ©ç”¨æ•™å¸ˆæ¨¡å‹ï¼ˆå¦‚ DeepSeek-R1ï¼‰æ ¹æ®é¢˜ç›®éš¾åº¦é‡å†™æ¨ç†é“¾ï¼Œç”Ÿæˆè‡ªé€‚åº”ä¸”ç®€æ´çš„æ¨ç†è·¯å¾„ï¼Œå¹¶æ„å»ºåŒ…å«10ä¸‡æ¡æ ·æœ¬çš„ LiteCoT æ•°æ®é›†ç”¨äºæ¨¡å‹å¾®è°ƒã€‚ |
</details>

#### 

<details>
<summary> Unlocking efficient long-to-short llm reasoning with model merging<a href="https://arxiv.org/abs/2503.20641" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Model merging offers a novel and efficient approach to reasoning. Wu et al. systematically explore various fusion strategies, including task vector-based fusion, singular value decomposition (SVD), and activation-based methods. Experimental results demonstrate the effectiveness of these strategies. | æ¨¡å‹èåˆæä¾›äº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„æ¨ç†æ–¹æ³•ã€‚Wu ç­‰äººç³»ç»Ÿåœ°æ¢ç´¢äº†å¤šç§èåˆç­–ç•¥ï¼ŒåŒ…æ‹¬åŸºäºä»»åŠ¡å‘é‡ã€å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å’Œæ¿€æ´»ä¿¡æ¯çš„èåˆæ–¹æ³•ï¼Œå®éªŒè¯æ˜è¿™äº›æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ |
</details>

#### 

<details>
<summary> Beyond â€˜Aha!â€™: Toward Systematic Meta-Abilities Alignment in Large Reasoning Models<a href="https://arxiv.org/abs/2505.10554" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes a three-stage framework for constructing reasoning LMs. In the first stage, multiple expert models are trained using modular RL, each specializing in a distinct reasoning paradigm such as deduction, induction, or abduction. Each expert is optimized via a reward function that combines format and answer correctness. In the second stage, the expert models are merged into a unified model using weighted parameter fusion. In the third stage, the merged model undergoes further fine-tuning on domain-specific tasks such as mathematics and programming, resulting in notable improvements in overall reasoning ability. This framework offers a viable paradigm for building efficient reasoning models. | è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µæ¨ç†èƒ½åŠ›æ„å»ºæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡æ¨¡å—åŒ–å¼ºåŒ–å­¦ä¹ åˆ†åˆ«è®­ç»ƒå…·å¤‡æ¼”ç»ã€å½’çº³å’Œæº¯å› ç­‰å…ƒèƒ½åŠ›çš„ä¸“å®¶æ¨¡å‹ï¼Œæ¯ä¸ªä¸“å®¶åœ¨å¯¹åº”æ•°æ®é›†ä¸ŠåŸºäºå¥–åŠ±å‡½æ•°ï¼ˆå¦‚â€œæ ¼å¼+ç­”æ¡ˆâ€å¾—åˆ†ï¼‰è¿›è¡Œä¼˜åŒ–ï¼›ç¬¬äºŒé˜¶æ®µï¼Œåˆ©ç”¨åŠ æƒå‚æ•°èåˆæ–¹æ³•å°†å„ä¸“å®¶æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ï¼Œå®éªŒè¡¨æ˜æ¼”ç»èƒ½åŠ›åœ¨æƒé‡ä¸­æ›´ä¸ºå…³é”®ï¼›ç¬¬ä¸‰é˜¶æ®µï¼Œåœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚æ•°å­¦ä¸ç¼–ç¨‹ï¼‰ä¸Šå¯¹èåˆæ¨¡å‹è¿›è¡Œå¼ºåŒ–è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡å…¶ç»¼åˆæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä¸ºåç»­æ„å»ºå…·å¤‡å¤šæ ·æ¨ç†æ¨¡å¼ï¼ˆå¦‚é•¿æ¨ç†ã€çŸ­æ¨ç†ã€è‡ªé€‚åº”æ¨ç†ï¼‰çš„ç»Ÿä¸€æ¨¡å‹æä¾›äº†å¯è¡Œè·¯å¾„ï¼Œå¹¶å¯ä¸æ¨¡å‹èåˆæ–¹æ³•ç›¸ç»“åˆè¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ã€‚ |
</details>

#### 

<details>
<summary> ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization<a href="https://arxiv.org/pdf/2506.10822" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes the **ReCUT** method, which generates multiple reasoning paths through progressive exploration and diverse sampling. Preference pairs are constructed based on reasoning accuracy and length, and two specialized models are trained separately using the DPO method. Finally, model merging is employed to obtain an efficient reasoning model, achieving an effective balance between reasoning quality and efficiency. | è¿™ç¯‡è®ºæ–‡æå‡ºäº†ReCUTæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡é€æ­¥æ¢ç´¢å’Œå¤šæ ·åŒ–é‡‡æ ·ç”Ÿæˆå¤šæ¡æ¨ç†è·¯å¾„ï¼Œé’ˆå¯¹æ¨ç†å‡†ç¡®æ€§ä¸æ¨ç†é•¿åº¦æ„å»ºåå¥½å­¦ä¹ å¯¹ï¼Œåˆ†åˆ«åˆ©ç”¨DPOæ–¹æ³•è®­ç»ƒä¸¤ä¸ªä¸“é—¨æ¨¡å‹ï¼Œæœ€ç»ˆé€šè¿‡æ¨¡å‹åˆå¹¶å¾—åˆ°é«˜æ•ˆæ¨ç†æ¨¡å‹ï¼Œå®ç°äº†æ¨ç†è´¨é‡ä¸æ•ˆç‡çš„æœ‰æ•ˆå¹³è¡¡ã€‚ |
</details>

#### 

<details>
<summary> Ada-R1: Hybrid CoT via Bi-Level Adaptive Reasoning Optimization<a href="https://arxiv.org/pdf/2504.21659" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **Ada-R1**, a two-stage adaptive reasoning framework. In the first stage, model merging is used to combine a reasoning model with a standard large language model, enabling the generation of both long and short reasoning paths. In the second stage, a biLevel adaptive reasoning mechanism is introduced: at the **group level**, a preference-guided model selects between long and short reasoning based on the input; at the **instance level**, preference optimization encourages the model to produce more concise reasoning while maintaining accuracy, thereby improving overall inference efficiency. | æœ¬æ–‡æå‡ºAda-R1ï¼Œä¸€ä¸ªä¸¤é˜¶æ®µçš„è‡ªé€‚åº”æ¨ç†æ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡æ¨¡å‹åˆå¹¶ï¼Œç»“åˆé•¿æ¨ç†æ¨¡å‹ä¸å¸¸è§„å¤§æ¨¡å‹ï¼Œä½¿å…¶å…·å¤‡ç”Ÿæˆé•¿çŸ­æ¨ç†è·¯å¾„çš„èƒ½åŠ›ï¼›ç¬¬äºŒé˜¶æ®µå¼•å…¥åŒå±‚è‡ªé€‚åº”æ¨ç†ä¼˜åŒ–æœºåˆ¶ï¼ŒåŒ…æ‹¬ç¾¤ä½“çº§åå¥½å¼•å¯¼æ¨¡å‹ä¾æ®è¾“å…¥é€‰æ‹©é•¿æ¨ç†è¿˜æ˜¯çŸ­æ¨ç†ï¼Œå®ä¾‹çº§åå¥½ä¿ƒä½¿æ¨¡å‹åœ¨ä¿æŒå‡†ç¡®æ€§çš„å‰æä¸‹ç”Ÿæˆæ›´ç®€æ´çš„æ¨ç†ï¼Œä»è€Œæå‡æ¨ç†æ•ˆç‡ã€‚ |
</details>

### Speculative Decoding

<details>
<summary> Reward-Guided Speculative Decoding for Efficient LLM Reasoning<a href="https://arxiv.org/pdf/2501.19324" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes the **Reward-Guided Speculative Decoding (RSD)** framework, which addresses the inefficiency of traditional speculative decoding under strict unbiasedness constraints. RSD employs a lightweight small model to generate candidate reasoning steps, evaluates them using a reward function, and invokes a large model for correction only when necessary, thereby enabling a dynamic trade-off between accuracy and computational cost. | æœ¬æ–‡æå‡ºäº†Reward-Guided Speculative Decoding (RSD)æ¡†æ¶ï¼Œçªç ´ä¼ ç»Ÿæ¨æµ‹è§£ç åœ¨æ— åæ€§çº¦æŸä¸‹æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚RSDåˆ©ç”¨è½»é‡å°æ¨¡å‹ç”Ÿæˆå€™é€‰æ¨ç†æ­¥éª¤ï¼Œå¹¶ç»“åˆå¥–åŠ±å‡½æ•°å¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼Œä»…åœ¨å¿…è¦æ—¶è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œä¿®æ­£ï¼Œä»è€Œå®ç°ç²¾åº¦ä¸è®¡ç®—æˆæœ¬ä¹‹é—´çš„åŠ¨æ€æƒè¡¡ã€‚ |
</details>

#### 

<details>
<summary> SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in Large Language Models<a href="https://arxiv.org/pdf/2505.07680" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| **SpecRouter** introduces a multi-stage, reasoning-aware decoding framework to replace the conventional static draftâ€“target model setup. This method dynamically selects the optimal draft model and intermediate verification path based on task complexity and system load, thereby improving reasoning efficiency and reducing refusal rates. | SpecRouteræå‡ºå¤šçº§æ¨ç†å¼è§£ç æ¡†æ¶ä»¥æ›¿ä»£ä¼ ç»Ÿé™æ€çš„è‰ç¨¿-ç›®æ ‡æ¨¡å‹æ­é…ã€‚è¯¥æ–¹æ³•å¯æ ¹æ®ä»»åŠ¡å¤æ‚åº¦ä¸ç³»ç»Ÿè´Ÿè½½åŠ¨æ€é€‰æ‹©æœ€ä¼˜è‰ç¨¿æ¨¡å‹åŠä¸­é—´éªŒè¯è·¯å¾„ï¼Œä¼˜åŒ–æ¨ç†æ•ˆç‡å¹¶é™ä½æ‹’ç»ç‡ã€‚ |
</details>

#### 

<details>
<summary> SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning<a href="https://arxiv.org/pdf/2504.07891" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes **SpecReason**, a speculative reasoning framework that adaptively delegates semantically simple and non-critical reasoning steps to a lightweight model at a fine-grained level. A stronger model then semantically verifies these steps; if verification passes, the reasoning proceeds, otherwise the stronger model takes over the reasoning process. | æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º SpecReason çš„æ¨æµ‹å¼æ¨ç†æ¡†æ¶ï¼ŒSpecReason é€šè¿‡ç»†ç²’åº¦ã€è‡ªé€‚åº”åœ°å°†è¯­ä¹‰ç®€å•ä¸”éå…³é”®çš„æ¨ç†æ­¥éª¤äº¤ç”±è½»é‡çº§æ¨¡å‹å®Œæˆï¼Œå†ç”±å¼ºæ¨¡å‹å¯¹è¿™äº›æ­¥éª¤è¿›è¡Œè¯­ä¹‰å±‚é¢çš„éªŒè¯ï¼Œè‹¥è¯„ä¼°é€šè¿‡åˆ™ç»§ç»­æ¨ç†ï¼Œå¦åˆ™ç”±å¼ºæ¨¡å‹æ¥æ›¿è¿›è¡Œæ¨ç†ã€‚ |
</details>

#### 

<details>
<summary>Speculative thinking: Enhancing small-model reasoning with large model guidance at inference time <a href="https://arxiv.org/pdf/2504.12329" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Speculative Thinking  dynamically identifies reflective, uncertain, or self-negating tokens in the draft generated by the SLM. The LLM selectively intervenes at these critical reasoning junctures, enhancing the quality of reasoning for complex tasks while preserving overall efficiency. | Speculative Thinkingé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€è¯†åˆ«å°æ¨¡å‹ç”Ÿæˆä¸­çš„åæ€ã€éªŒè¯æˆ–åå¤å¦å®šç­‰ä¿¡å·ï¼Œé€‰æ‹©æ€§åœ°ç”±å¤§æ¨¡å‹ä»‹å…¥å…³é”®æ­¥éª¤ï¼Œä»¥å¢å¼ºå¤æ‚ä»»åŠ¡ä¸­çš„æ¨ç†è´¨é‡ã€‚ |
</details>

#### 

<details>
<summary> Efficient Reasoning for LLMs through Speculative Chain-of-Thought<a href="https://arxiv.org/pdf/2504.19095" target="_blank">
    [Paper]
</a></summary>



| English Note                                                 | Chinese Note                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| This paper proposes a Speculative Chain-of-Thought (SCoT) framework based on collaboration between large and small models. The method employs a lightweight draft model to generate preliminary reasoning chains, which are then selected and corrected by the target model, effectively improving reasoning efficiency and speed. | æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§æ¨¡å‹ä¸å°æ¨¡å‹ååŒçš„Speculative Chain-of-Thoughtï¼ˆSCoTï¼‰æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡è½»é‡çº§è‰ç¨¿æ¨¡å‹è¿›è¡Œæ€ç»´é“¾è‰æ‹Ÿï¼Œå¹¶åˆ©ç”¨ç›®æ ‡æ¨¡å‹å¯¹è‰ç¨¿è¿›è¡Œé€‰æ‹©å’Œçº é”™ï¼Œæœ‰æ•ˆæå‡äº†æ¨ç†æ•ˆç‡ä¸é€Ÿåº¦ã€‚ |
</details>



# ğŸ” Citation 


# ğŸ§© Acknowledgements

This repository is inspired by [Awesome-Efficient-Reasoning](https://github.com/hemingkx/Awesome-Efficient-Reasoning),  [Awesome-Hybrid-CoT-Reasoning](https://github.com/StarDewXXX/Awesome-Hybrid-CoT-Reasoning), [Awesome-Large-Multimodal-Reasoning-Models](https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models). Many thanks for their contribution!




