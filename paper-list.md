# üìö All Papers 

## üåü Efficient Reasoning with Single Model

Efficient reasoning with single model aims to achieve efficient reasoning by optimizing the reasoning process of a single model. This approach focuses on minimizing computational resources and reasoning time while maintaining reasoning accuracy, ensuring that the model can quickly and accurately generate answers. Specific methods include Early Exit, CoT Compression, Adaptive Reasoning , and Representation Engineering-based Efficient Reasoning.

### Early Exit

  <summary> To Think or Not to Think: Exploring the Unthinking Vulnerability in Large Reasoning Models <a href="https://arxiv.org/pdf/2502.12202" target="_blank">
    [Paper]
</a></summary>


<summary> Dynamic Early Exit in Reasoning Models <a href="https://arxiv.org/pdf/2504.15895" target="_blank">
    [Paper]
</a></summary>


<summary>ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning <a href="https://arxiv.org/pdf/2505.04881" target="_blank">
    [Paper]
</a></summary>



<summary> Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens<a href="https://arxiv.org/pdf/2505.18237" target="_blank">
    [Paper]
</a></summary>





<summary> Scalable Chain of Thoughts via Elastic Reasoning<a href="https://arxiv.org/pdf/2505.05315" target="_blank">
    [Paper]
</a></summary>


<summary> Answer Convergence as a Signal for Early Stopping in Reasoning<a href="https://arxiv.org/pdf/2506.02536" target="_blank">
    [Paper]
</a></summary>






<summary> Reasoning Models Know When They‚Äôre Right: Probing Hidden States for Self-Verification<a href="https://arxiv.org/pdf/2504.05419" target="_blank">
    [Paper]
</a></summary>




<summary>FlashThink: An Early Exit Method For Efficient Reasoning <a href="https://arxiv.org/pdf/2505.13949" target="_blank">
    [Paper]
</a></summary>




<summary> Wait, We Don‚Äôt Need to ‚ÄúWait‚Äù ! Removing Thinking Tokens Improves Reasoning Efficiency<a href="https://arxiv.org/pdf/2506.08343" target="_blank">
    [Paper]
</a></summary>



<summary> Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs<a href="https://arxiv.org/pdf/2501.18585" target="_blank">
    [Paper]
</a></summary>



<summary> Efficient Reasoning Through Suppression of Self-Affirmation Reflections in Large Reasoning Models <a href="https://www.arxiv.org/pdf/2506.12353" target="_blank">
    [Paper]
</a></summary>



<summary>S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models <a href="https://arxiv.org/pdf/2505.07686" target="_blank">
    [Paper]
</a></summary>



### CoT Compression


<summary>Not All Tokens Are What You Need In Thinking <a href="https://arxiv.org/pdf/2505.17827" target="_blank">
    [Paper]
</a></summary>



<summary>TokenSkip: Controllable Chain-of-Thought Compression in LLMs <a href="https://arxiv.org/pdf/2502.12067" target="_blank">
    [Paper]
</a></summary>






<summary> Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning<a href="https://arxiv.org/pdf/2505.13866" target="_blank">
    [Paper]
</a></summary>






<summary>Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping <a href="https://arxiv.org/pdf/2505.08392" target="_blank">
    [Paper]
</a></summary>






<summary> LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling<a href="https://arxiv.org/pdf/2505.19187" target="_blank">
    [Paper]
</a></summary>






<summary> Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models<a href="https://arxiv.org/pdf/2502.13260" target="_blank">
    [Paper]
</a></summary>






<summary> R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search<a href="https://arxiv.org/pdf/2505.16838" target="_blank">
    [Paper]
</a></summary>




<summary> Don‚Äôt Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models<a href="https://arxiv.org/pdf/2505.21765" target="_blank">
    [Paper]
</a></summary>



<summary> Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning<a href="https://arxiv.org/pdf/2505.14582" target="_blank">
    [Paper]
</a></summary>



<summary> A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings<a href="https://arxiv.org/pdf/2505.24550" target="_blank">
    [Paper]
</a></summary>



<summary>Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models <a href="https://arxiv.org/pdf/2505.03469" target="_blank">
    [Paper]
</a></summary>



<summary> AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models<a href="https://arxiv.org/pdf/2505.22662" target="_blank">
    [Paper]
</a></summary>




<summary> Self-Training Elicits Concise Reasoning in Large Language Models<a href="https://arxiv.org/pdf/2502.20122" target="_blank">
    [Paper]
</a></summary>




<summary> Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models<a href="https://arxiv.org/pdf/2506.04210" target="_blank">
    [Paper]
</a></summary>




<summary> SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models<a href="https://arxiv.org/pdf/2506.05745" target="_blank">
    [Paper]
</a></summary>


<summary> Optimizing Length Compression in Large Reasoning Models<a href="https://arxiv.org/pdf/2506.14755" target="_blank">
    [Paper]
</a></summary>



<summary> Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition<a href="https://arxiv.org/pdf/2505.19788" target="_blank">
    [Paper]
</a></summary>



<summary>Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching <a href="https://arxiv.org/pdf/2503.05179" target="_blank">
    [Paper]
</a></summary>



<summary> Don‚Äôt Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning<a href="https://arxiv.org/abs/2505.17813" target="_blank">
    [Paper]
</a></summary>



### Adaptive Reasoning


<summary>Think Only When You Need with Large Hybrid-Reasoning Models <a href="https://arxiv.org/pdf/2505.14631" target="_blank">
    [Paper]
</a></summary>



<summary> Adaptive Deep Reasoning: Triggering Deep Thinking When Needed<a href="https://arxiv.org/pdf/2505.20101" target="_blank">
    [Paper]
</a></summary>



<summary>Ada-R1: Hybrid CoT via Bi-Level Adaptive Reasoning Optimization <a href="https://arxiv.org/pdf/2504.21659" target="_blank">
    [Paper]
</a></summary>



<summary> Dast: Difficulty-adaptive slow-thinking for large reasoning models<a href="https://arxiv.org/pdf/2503.04472" target="_blank">
    [Paper]
</a></summary>



<summary> Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence<a href="https://arxiv.org/pdf/2505.20325" target="_blank">
    [Paper]
</a></summary>



<summary> Thinker: Learning to Think Fast and Slow<a href="https://arxiv.org/pdf/2505.21097" target="_blank">
    [Paper]
</a></summary>



<summary> Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning<a href="https://arxiv.org/pdf/2505.16315" target="_blank">
    [Paper]
</a></summary>



<summary> AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting<a href="https://arxiv.org/pdf/2505.18822" target="_blank">
    [Paper]
</a></summary>


<summary> Thinkless: LLM Learns When to Think<a href="https://arxiv.org/pdf/2505.13379" target="_blank">
    [Paper]
</a></summary>




<summary> Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL<a href="https://arxiv.org/pdf/2505.10832" target="_blank">
    [Paper]
</a></summary>





<summary>AdaptThink: Reasoning Models Can Learn When to Think <a href="https://arxiv.org/pdf/2505.13417" target="_blank">
    [Paper]
</a></summary>






<summary> AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning<a href="https://arxiv.org/pdf/2505.11896" target="_blank">
    [Paper]
</a></summary>





<summary> Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning<a href="https://arxiv.org/pdf/2505.15154" target="_blank">
    [Paper]
</a></summary>





<summary>ARM: Adaptive Reasoning Model <a href="https://arxiv.org/pdf/2505.20258" target="_blank">
    [Paper]
</a></summary>





<summary> Interleaved Reasoning for Large Language Models via Reinforcement Learning<a href="https://arxiv.org/abs/2505.19640" target="_blank">
    [Paper]
</a></summary>




<summary> How Far Are We from Optimal Reasoning Efficiency?<a href="https://arxiv.org/pdf/2506.07104" target="_blank">
    [Paper]
</a></summary>




<summary>AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control <a href="https://arxiv.org/pdf/2506.20160" target="_blank">
    [Paper]
</a></summary>






<summary> O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning<a href="https://arxiv.org/abs/2501.12570" target="_blank">
    [Paper]
</a></summary>




<summary> Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning<a href="https://arxiv.org/pdf/2504.01296" target="_blank">
    [Paper]
</a></summary>



<summary>Learn to Reason Efficiently with Adaptive Length-based Reward Shaping <a href="https://arxiv.org/pdf/2505.15612" target="_blank">
    [Paper]
</a></summary>



<summary>ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning <a href="https://arxiv.org/pdf/2504.21370" target="_blank">
    [Paper]
</a></summary>



<summary> Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning<a href="https://arxiv.org/pdf/2506.05256" target="_blank">
    [Paper]
</a></summary>



<summary> SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning<a href="https://arxiv.org/pdf/2505.11274" target="_blank">
    [Paper]
</a></summary>



<summary> When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning<a href="https://arxiv.org/pdf/2505.15400" target="_blank">
    [Paper]
</a></summary>



<summary>TL;DR: Too Long, Do Re-weighting for Efficient LLM Reasoning Compression <a href="https://arxiv.org/pdf/2506.02678" target="_blank">
    [Paper]
</a></summary>

<summary>Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards  <a href="https://arxiv.org/abs/2505.18298" target="_blank">
    [Paper]
</a></summary>
<summary>OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation <a href="https://arxiv.org/abs/2506.02397 " target="_blank">
    [Paper]
</a></summary>
<summary>Think When You Need: Self-Adaptive Chain-of-Thought Learning <a href="https://arxiv.org/abs/2504.03234 " target="_blank">
    [Paper]
</a></summary>
<summary>Efficient RL Training for Reasoning Models via Length-Aware Optimization <a href="https://arxiv.org/abs/2505.12284" target="_blank">
    [Paper]
</a></summary>

<summary>Optimizing Anytime Reasoning via Budget Relative Policy Optimization <a href="https://arxiv.org/abs/2505.13438" target="_blank">
    [Paper]
</a></summary>

<summary>Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning <a href="https://arxiv.org/abs/2505.21178" target="_blank">
    [Paper]
</a></summary>

<summary>Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning <a href="https://arxiv.org/abs/2506.08125" target="_blank">
    [Paper]
</a></summary>

<summary>Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty <a href="https://arxiv.org/abs/2506.10446 " target="_blank">
    [Paper]
</a></summary>

<summary>Training Language Models to Reason Efficiently <a href="https://arxiv.org/abs/2502.04463 " target="_blank">
    [Paper]
</a></summary>


### Representation Engineering based Efficient Reasoning


<summary>Steerable Reasoning Calibration of Large Language Models for Free <a href="https://arxiv.org/pdf/2504.07986" target="_blank">
    [Paper]
</a></summary>


<summary> On Reasoning Strength Planning in Large Reasoning Models<a href="https://arxiv.org/pdf/2506.08390" target="_blank">
    [Paper]
</a></summary>



<summary> CoT-Valve: Length-Compressible Chain-of-Thought Tuning<a href="https://arxiv.org/pdf/2502.09601" target="_blank">
    [Paper]
</a></summary>



<summary>Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs <a href="https://arxiv.org/pdf/2506.07240" target="_blank">
    [Paper]
</a></summary>



<summary>Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute <a href="https://arxiv.org/pdf/2506.15882" target="_blank">
    [Paper]
</a></summary>


<summary>Mitigating Overthinking in Large Reasoning Models via Manifold Steering <a href="https://arxiv.org/pdf/2505.22411" target="_blank">
    [Paper]
</a></summary>

<summary>Activation Steering for Chain-of-Thought Compression <a href="https://arxiv.org/abs/2507.04742" target="_blank">
    [Paper]
</a></summary>

<summary>Controlling Thinking Speed in Reasoning Models <a href="https://arxiv.org/abs/2507.03704" target="_blank">
    [Paper]
</a></summary>


## üåü Efficient Reasoning with Model Collaboration

Efficient reasoning with model collaboration aims to enhance reasoning efficiency and accuracy in LLMs by enabling cooperation between multiple LLMs, each leveraging distinct reasoning strengths. Unlike single model efficient reasoning method, collaborative frameworks strategically combine long-chain reasoning models (long CoT) that excel at handling complex tasks and short-chain reasoning models (short CoT) that are lightweight and efficient for general tasks. This synergy allows for more fine-grained and cost-effective control of the reasoning process. Specific methods include Long‚ÄìShort Model Collaboration, LLM Routing, Model Consolidation, and Speculative Decoding.


### Long‚ÄìShort Model Collaboration


<summary> SplitReason: Learning To Offload Reasoning<a href="https://arxiv.org/pdf/2504.16379" target="_blank">
    [Paper]
</a></summary>




<summary>Thought manipulation: External thought can be efficient for large reasoning models <a href="https://arxiv.org/pdf/2504.13626" target="_blank">
    [Paper]
</a></summary>



<summary>CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models <a href="https://arxiv.org/pdf/2505.22017" target="_blank">
    [Paper]
</a></summary>




<summary> Plan and Budget: Effective and Efficient Test-Time Scaling on Large Language Model Reasoning<a href="https://arxiv.org/abs/2505.16122" target="_blank">
    [Paper]
</a></summary>






<summary>VeriThinker: Learning to Verify Makes Reasoning Model Efficient <a href="https://arxiv.org/pdf/2505.17941" target="_blank">
    [Paper]
</a></summary>






<summary> Guiding Reasoning in Small Language Models with LLM Assistance<a href="https://arxiv.org/pdf/2504.09923" target="_blank">
    [Paper]
</a></summary>





<summary>What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding <a href="https://arxiv.org/pdf/2506.06998" target="_blank">
    [Paper]
</a></summary>



<summary> Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning<a href="https://arxiv.org/pdf/2505.11827" target="_blank">
    [Paper]
</a></summary>


<summary> Collaborative LLM Inference via Planning for Efficient Reasoning<a href="https://arxiv.org/pdf/2506.11578" target="_blank">
    [Paper]
</a></summary>




<summary> ThinkSwitcher: When to Think Hard, When to Think Fast<a href="https://arxiv.org/pdf/2505.14183" target="_blank">
    [Paper]
</a></summary>



### LLM Routing


<summary>Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning <a href="https://arxiv.org/pdf/2505.20664" target="_blank">
    [Paper]
</a></summary>




<summary>Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced Model Router <a href="https://arxiv.org/pdf/2506.05901" target="_blank">
    [Paper]
</a></summary>





<summary> TagRouter: Learning Route to LLMs through Tags for Open-Domain Text Generation Tasks<a href="https://www.arxiv.org/abs/2506.12473" target="_blank">
    [Paper]
</a></summary>






<summary> Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models<a href="https://arxiv.org/pdf/2506.04182" target="_blank">
    [Paper]
</a></summary>






<summary>Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning <a href="https://arxiv.org/pdf/2506.09033" target="_blank">
    [Paper]
</a></summary>






<summary>R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing <a href="https://arxiv.org/pdf/2505.21600" target="_blank">
    [Paper]
</a></summary>



<summary>Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection <a href="https://arxiv.org/abs/2505.19435" target="_blank">
    [Paper]
</a></summary>
<summary>RouteLLM: Learning to Route LLMs from Preference Data <a href="https://arxiv.org/abs/2406.18665 " target="_blank">
    [Paper]
</a></summary>
<summary>IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory <a href="https://arxiv.org/abs/2506.01048" target="_blank">
    [Paper]
</a></summary>
<summary>Harnessing multiple large language models: A survey on llm ensemble <a href="https://arxiv.org/abs/2502.18036 " target="_blank">
    [Paper]
</a></summary>
<summary>Graphrouter: A graph-based router for llm selections <a href="https://arxiv.org/abs/2410.03834" target="_blank">
    [Paper]
</a></summary>
<summary>ROUTERBENCH: A Benchmark for Multi-LLM Routing System <a href="https://arxiv.org/abs/2403.12031 " target="_blank">
    [Paper]
</a></summary>
<summary>Large language model routing with benchmark datasets <a href="https://arxiv.org/abs/2309.15789 " target="_blank">
    [Paper]
</a></summary>
<summary>Routing to the expert: Efficient reward-guided ensemble of large language models <a href="https://arxiv.org/abs/2311.08692" target="_blank">
    [Paper]
</a></summary>
<summary>Routerdc: Query-based router by dual contrastive learning for assembling large language models <a href="https://arxiv.org/abs/2409.19886" target="_blank">
    [Paper]
</a></summary>
<summary>Hybrid llm: Cost-efficient and quality-aware query routing <a href="https://arxiv.org/abs/2404.14618 " target="_blank">
    [Paper]
</a></summary>
<summary>EmbedLLM: Learning Compact Representations of Large Language Models <a href="https://arxiv.org/abs/2410.02223" target="_blank">
    [Paper]
</a></summary>

  
### Model Consolidation


<summary>Thinking without Tokens by Habitual Reasoning Distillation with Multi-Teachers‚Äô Guidance <a href="https://arxiv.org/pdf/2503.24198" target="_blank">
    [Paper]
</a></summary>






<summary> DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models <a href="https://arxiv.org/pdf/2505.13975" target="_blank">
    [Paper]
</a></summary>



<summary> Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting <a href="https://arxiv.org/pdf/2505.19716" target="_blank">
    [Paper]
</a></summary>


<summary> Unlocking efficient long-to-short llm reasoning with model merging <a href="https://arxiv.org/abs/2503.20641" target="_blank">
    [Paper]
</a></summary>



<summary> Beyond ‚ÄòAha!‚Äô: Toward Systematic Meta-Abilities Alignment in Large Reasoning Models <a href="https://arxiv.org/abs/2505.10554" target="_blank">
    [Paper]
</a></summary>


<summary> ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization <a href="https://arxiv.org/pdf/2506.10822" target="_blank">
    [Paper]
</a></summary>



<summary> Ada-R1: Hybrid CoT via Bi-Level Adaptive Reasoning Optimization <a href="https://arxiv.org/pdf/2504.21659" target="_blank">
    [Paper]
</a></summary>

<summary> Hawkeye:Efficient Reasoning with Model Collaboration <a href="https://arxiv.org/pdf/2504.00424" target="_blank">
    [Paper]
</a></summary>

<summary> Kimi k1. 5: Scaling reinforcement learning with llms <a href="https://arxiv.org/abs/2501.12599" target="_blank">
    [Paper]
</a></summary>


<summary> Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond <a href="https://arxiv.org/abs/2503.10460" target="_blank">
    [Paper]
</a></summary>




### Speculative Decoding


<summary> Reward-Guided Speculative Decoding for Efficient LLM Reasoning<a href="https://arxiv.org/pdf/2501.19324" target="_blank">
    [Paper]
</a></summary>





<summary> SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in Large Language Models<a href="https://arxiv.org/pdf/2505.07680" target="_blank">
    [Paper]
</a></summary>





<summary> SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning<a href="https://arxiv.org/pdf/2504.07891" target="_blank">
    [Paper]
</a></summary>






<summary>Speculative thinking: Enhancing small-model reasoning with large model guidance at inference time <a href="https://arxiv.org/pdf/2504.12329" target="_blank">
    [Paper]
</a></summary>





<summary> Efficient Reasoning for LLMs through Speculative Chain-of-Thought<a href="https://arxiv.org/pdf/2504.19095" target="_blank">
    [Paper]
</a></summary>





  <summary> To Think or Not to Think: Exploring the Unthinking Vulnerability in Large Reasoning Models <a href="https://arxiv.org/pdf/2502.12202" target="_blank">
    [Paper]
</a></summary>



 <summary> Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding  <a href="https://arxiv.org/abs/2401.07851 " target="_blank">
    [Paper]
</a></summary>
 <summary> Pearl: Parallel speculative decoding with adaptive draft length <a href="https://arxiv.org/abs/2408.11850 " target="_blank">
    [Paper]
</a></summary>
 <summary> POSS: Position Specialist Generates Better Draft for Speculative Decoding <a href="https://arxiv.org/abs/2506.03566 " target="_blank">
    [Paper]
</a></summary>
 <summary> Accelerating Large Language Model Reasoning via Speculative Search <a href="https://arxiv.org/abs/2505.02865" target="_blank">
    [Paper]
</a></summary>


